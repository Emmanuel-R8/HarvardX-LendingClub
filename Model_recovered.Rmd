---
title: "R Notebook"
output: html_notebook
---

> WARNING: To run this model, you will need at least 32GB of memory (real plus virtual/swap space) 

# Weight of evidence binning

## Data preparation

```{r child="CleanLoad.Rmd"}

```

```{r}
# Not used later on
rm(loans)
```


```{r}
# dplyr::select the variables checked in 01-startup.Rmd
varList <- c(LC_variable[LC_variable$inModel == TRUE, "variable_name"])$variable_name
varList <- c(varList, "grade_num", "sub_grade_num", "principal_loss_pct", "creditMargin", "monthDefault")

# Make sure that some variables are NOT in included in the final training set
varRemove <- c("grade_num", "sub_grade_num", "principal_loss_pct", "creditMargin", "monthDefault")
```


```{r}
#################################################################################################
##
## Prepare a dataset with ONLY the predictors NOT removing NA's
##
loansPredictors <-
  loansWorkingSet %>%

  # Keep the chosen predictors
  # Use tidyselect::one_of() to avoid errors if column does not exist
  dplyr::select(tidyselect::one_of(varList)) %>%

  ##
  ## Dates to numeric, in 'decimal' years since 2000
  ##
  dplyr::mutate_at(c("issue_d", "earliest_cr_line"), function(d) {
    return(year(d) - 2000 + (month(d) - 1) / 12)
  }) %>%

  ## Add polynomials of the dates to model the time-trend shape
  dplyr::mutate(
    issue_d2 = issue_d^2,
    issue_d3 = issue_d^3,
    earliest_cr_line2 = earliest_cr_line^2,
    earliest_cr_line3 = earliest_cr_line^3
  ) %>%

  ## Create a logical flag TRUE for non-defaulted (good) loans
  dplyr::mutate(isGoodLoan = (principal_loss_pct < 0.001)) %>%

  dplyr::select(-tidyselect::one_of(varRemove))



#################################################################################################
##
## Create training / test sets 80%/20%
##
proportionTraining <- 0.8
set.seed(42)
sampleTraining <- sample(1:nSamples, floor(nSamples * proportionTraining), replace = FALSE)
loansTraining <- loansPredictors %>% dplyr::slice(sampleTraining)
loansTest <- loansPredictors %>% dplyr::slice(-sampleTraining)

# Subsets of the training set
set.seed(42)
nSamplesTraining <- nrow(loansTraining)
sample20 <- sample(1:nSamplesTraining, floor(nSamplesTraining * 0.20), replace = FALSE)

loans20 <- loansTraining %>% dplyr::slice(sample20)
```

```{r}
# Not used later on
rm(loansWorkingSet, loansPredictors)
```

```{r}
# To speedup
listIV        <- readRDS("datasets/listIV100.rds")
listBins      <- readRDS("datasets/listBins100.rds")
bestBins      <- readRDS("datasets/bestBins100.rds")
allCategories <- readRDS("datasets/allCategories100.rds")
SGLMtrain     <- readRDS("datasets/modelSGLM100.rds")
SGLMretrain   <- readRDS("datasets/modelSGLM100retrain.rds")

```

## Binning

```{r}
library(binner)
```

```{r reinstall-binner,eval=FALSE}
detach("package:binner", unload = TRUE, force = TRUE)
devtools::install_local("~/Development/R/DVPT-PACKAGES/Score_modeling_binning/binner", force = TRUE, quiet = TRUE)
# devtools::install_github("Emmanuel-R8/SMBinning")
library(binner)
```


```{r}
loansBinning <- loans20 %>%
  dplyr::mutate(
    home_ownership = as_factor(home_ownership),
    emp_length = as_factor(emp_length)
  )
```

### dti

```{r}
resultHome <- WoETableContinuous(
  df = loansBinning,
  x = "dti",
  y = "isGoodLoan",
  p = 0.05
)

resultHome$table
resultHome$IV
```

### Home ownership

```{r}
resultHome <- WoETable(
  df = loansBinning,
  x = "home_ownership",
  y = "isGoodLoan",
  maxCategories = 20
)

resultHome
resultHome$IV
```

### Verification Status

```{r}
resultVerif <- WoETable(
  df = loansBinning,
  x = "verification_status",
  y = "isGoodLoan",
  maxCategories = 20
)

resultVerif
resultVerif$IV
```



### Purpose

```{r}
resultHome <- WoETable(
  df = loansBinning,
  x = "purpose",
  y = "isGoodLoan",
  maxCategories = 20
)

resultHome
resultHome$IV
```


### Sub grade

```{r}
resultSubgrade <- WoETable(
  df = loansBinning,
  x = "sub_grade",
  y = "isGoodLoan",
  maxCategories = 50
)

resultSubgrade
resultSubgrade$IV
```



### Employment years

```{r}
resultEmp <- WoETable(
  df = loansBinning,
  x = "emp_length",
  y = "isGoodLoan",
  maxCategories = 20
)

resultEmp
resultEmp$IV
```



### Bank card utilisation

```{r}
resultBC <- WoETable(
  df = loansBinning,
  x = "bc_util",
  y = "isGoodLoan",
  p = 0.05
)

resultBC
resultBC$IV
```

### State

```{r}
resultState <- WoETable(
  df = loansBinning,
  x = "addr_state",
  y = "isGoodLoan",
  maxCategories = 100
)

resultState
resultState$IV
```


### Annual income

```{r}
which(names(loansBinning) == "annual_inc_joint")

resultInc <- WoETable(
  df = loansBinning,
  x = "annual_inc_joint",
  y = "isGoodLoan",
  p = 0.05
)

resultHome
resultHome$iv
```

### Loan amount

```{r}
resultAmnt <- WoETable(
  df = loansBinning,
  x = "loan_amnt",
  y = "isGoodLoan",
  p = 0.05
)

resultAmnt
resultAmnt$IV
```

# Model using all samples




## Loop through all variables

```{r}
loansBinning <- loansTraining %>%
  dplyr::mutate(
    home_ownership = as_factor(home_ownership),
    emp_length = as_factor(emp_length),
    grade = as_factor(grade)
  )

listIV <- dplyr::tibble(variable = names(loansBinning), IV = -1.0)
listBins <-
  dplyr::tibble(
    variable = "",
    type = "",
    IV = 0.0,
    WoE = list(),
    .rows = 0
  )

# About 500 sec wall-time
startTime <- proc.time()
for (n in listIV$variable) {
  cat("Variable: ", n)

  # We don't test the response with itself
  if (n %in% c("isGoodLoan")) {
    cat("\n")

  } else {
    if (class(loansBinning[[1, n]]) == "factor") {
      cat(" is a factor, ")
      result <- WoETableCategorical(
        df = loansBinning,
        x = n,
        y = "isGoodLoan",
        maxCategories = 100)

    } else {
      cat(" is numeric, ")
      result <- WoETableContinuous(df = loansBinning,
                                   x = n,
                                   y = "isGoodLoan",
                                   p = 0.05)
    }

    tryCatch({
      if (is.na(result)) {
        cat("Variable skipped.\n")
        dplyr::add_row(
          listBins,
          variable = n,
          type = NA,
          IV = NA
        )
      } else {
        cat("     IV : ", result$IV, "\n")

        listBins <- listBins %>%
          dplyr::add_row(
            variable = n,
            type = result$type,
            IV = result$IV,
            WoE = list(result$table)
          )
        
        listIV[n] <- result$IV
        
        }
    },
    finally = {})
  }
}
proc.time() - startTime

# saveRDS(listIV, "datasets/listIV100.rds")
# saveRDS(listBins, "datasets/listBins100.rds")

```

## Select relevant variables

Ignore the variables that would not be available at the time the credit scoring is performed.

```{r}

# This shows that almost all variables have an information value above 2% which is considered the threshold to be weakly
# useful. Useless variables are all related to F and G ratings which we do not use anyway.
for (i in 1:length(listBins$variable)) {
  useless <- listBins$WoE[i] %>% as.data.frame() %>% filter(IV <= 0.02)
  if (nrow(useless) > 0) print(useless)
}

bestBins <- listBins %>%
  arrange(desc(IV)) %>%
  filter(!(variable %in% c(
    "loanID", "term", "int_rate", "creditMargin", "loan_status",
    "grade", "sub_grade", "grade_num", "sub_grade_num",
    "emp_length", "home_ownership", "monthDefault",
    "principal_loss_pct", "creditMargin", "monthDefault",
    "isGoodLoan")))


bestBins

# saveRDS(bestBins, "datasets/bestBins100.rds")

```

## Create data table with only binary variables (transform every bin to a 0/1 value)

For each variable, create new variables for each bin in the WoE table of that variable.

```{r}
# Variable will contain all the categories for all the variables.
# WARNING: 3.3 GB dataframe
allCategories <- loansTraining[,"loanID"]

for (index in 1:length(bestBins$variable)) {

  binned <- categoriseFromWoE(df = loansTraining,
                              varName =  bestBins$variable[index],
                              woeTable = bestBins$WoE[[index]])

  allCategories <- cbind(allCategories, binned)
}

# saveRDS(allCategories, "datasets/allCategories100.rds")
```

## Logistic regression using the `SpeedGLM` package

> WARNING: `speedglm` has a `select()` function which shadows `dplyr::select()`. From there on, functions are fully qualified to avoid any collision.

### First training on variables previously selected on their Information Value

```{r}
# WARNING: 3.3 GB dataframe but no allocation (pointers to objects in allCategories) until modifications
# Start at 2 to remove `loanID`
# Add the desired response
loanSample <- 
  allCategories[, 2:length(names(allCategories))] %>%
  cbind(loansTraining$isGoodLoan) %>%
  dplyr::rename(isGoodLoan = "loansTraining$isGoodLoan") %>%
  dplyr::mutate(isGoodLoan = if_else(isGoodLoan, 1, 0)) %>%
  as.data.frame()
ncol(loanSample)


# Running GLM would give a number of NAs if some columns are linearly dependant (in our case actually equal).
# A "trick" to identify them is to run a hash digest on each column and spot identical hashes
# The original variables were not identical, but when split into categories, they can become identical. 
# For example, if no income is provided (income=NA category), no debt-to-income ratio can be calculated (dti=NA).
duplicateNames <- loanSample[duplicated(lapply(loanSample, digest::digest))] %>% names()
duplicateNames
```


```{r}
# We will remove those variables from the dataset.
loanSample <- loanSample %>% select(-one_of(duplicateNames))

# Just to check that there are no further duplicates
# duplicateNames <- loanSample[duplicated(lapply(loanSample, digest::digest))] %>% names()

# Remove any categories uniformaly constant (only 0)
# This is important when working on a small extract of the dataset for variables that are seldom used (e.g. customers
# from certain states).
loanSample <- loanSample[, colSums(loanSample) != 0]

ncol(loanSample)
```


```{r}
# About 700 sec wall-time to complete training dataset
{
  startTime <- proc.time()
  SGLMtrain <- speedglm::speedglm(isGoodLoan ~ ., data = loanSample, family = binomial(), intercept = TRUE)
  cat(proc.time() - startTime)
}

summary(SGLMtrain)


```


```{r}
# Does the model throw NAs?. Let us select those variables:
NANames <- names(loanSample)[is.na(summary(SGLMtrain)$coefficient$Estimate)] 

# Just in case `isGoodLoan` was selected (possible in theory on small dataset extract)
NANames <- NANames[NANames != "isGoodLoan"]

# Remove those NA variables.
loanSample <- loanSample %>% select(-one_of(NANames))
ncol(loanSample)
```


```{r}
# Start the model training again
if (length(NANames) > 0) {
  startTime <- proc.time()
  SGLMtrain <- speedglm::speedglm(isGoodLoan ~ ., data = loanSample, family = binomial(), intercept = TRUE)
  cat(proc.time() - startTime)
}

summary(SGLMtrain)


# saveRDS(SGLMtrain, "datasets/modelSGLM100.rds")
```


```{r}
# List of model variables
modelNames <- attr(SGLMtrain$coefficients, "names") %>%
  tibble::enframe() %>%
  dplyr::rename(modelName = "value")


# and their coefficients in the model + z-value (= p values)
listVars <- summary(SGLMtrain)$coefficients %>%
  dplyr::as_tibble() %>%
  cbind(modelNames) %>%
  dplyr::rename(zValue = "z value")
```


# Model performance

Using the model is a simple matrix multiplication: $\text{Loan matrix} \times \text{Scorecard weights}$

## Training set

```{r}
# Create a matrix out of the dataset 
# We start with the entore dataset, then remove every variable that is not in the list of variables in the model
allMatrix <- allCategories[, !is.na(match(names(allCategories), str_remove_all(listVars$modelName, "\`")))] %>% as.matrix()
dim(allMatrix)

# Cleaning is necessary to not exceed 32GB of memory usage and subsequent crash...
rm(loanSample, allCategories)

scorecardVector <- listVars$scoreCard %>% as.matrix()
estimateVector <- listVars$Estimate %>% as.matrix()

dim(scorecardVector)

outcomeScores <- allMatrix %*% scorecardVector
outcomeEstimates <- allMatrix %*% estimateVector


loansTraining %>%
  dplyr::select(sub_grade) %>%
  cbind(outcomeScores %>% as_tibble()) %>%
  dplyr::rename(Score = V1) %>%
  group_by(sub_grade) %>%
  summarise(Mean = mean(Score))

loansTraining %>%
  dplyr::select(sub_grade) %>%
  cbind(outcomeEstimates %>% as_tibble()) %>%
  dplyr::rename(Estimate = V1) %>%
  group_by(sub_grade) %>%
  summarise(Mean = mean(Score))


loansTraining %>%
  dplyr::select(sub_grade) %>%
  cbind(outcomeScores %>% as_tibble()) %>%
  dplyr::rename(Score = V1) %>%
  dplyr::sample_n(50000) %>%
  ggplot(aes(sub_grade, Score)) +
  geom_point(alpha = 0.1, col = "blue")

loansTraining %>%
  dplyr::select(sub_grade) %>%
  cbind(outcomeEstimates %>% as_tibble()) %>%
  dplyr::rename(Estimate = V1) %>%
  dplyr::sample_n(50000) %>%
  ggplot(aes(sub_grade, Estimate)) +
  geom_point(alpha = 0.1, col = "blue")
```


## Test set

```{r}
# Prepare the full test set
predictionCategories <- loansTest[,"loanID"]

for (index in 1:length(bestBins$variable)) {

  binned <- categoriseFromWoE(df = loansTest,
                              varName =  bestBins$variable[index],
                              woeTable = bestBins$WoE[[index]])

  predictionCategories <- cbind(predictionCategories, binned)
}

# Retain only the relevant scorecard categories
predictionCategories <- predictionCategories[, 2:length(names(predictionCategories))] %>%
  dplyr::select(tidyselect::one_of(selectedCategories)) %>%
  cbind(loansTest$isGoodLoan) %>%
  dplyr::rename(isGoodLoan = "loansTest$isGoodLoan") %>%
  dplyr::mutate(isGoodLoan = if_else(isGoodLoan, 1, 0)) %>%
  as.data.frame()

dim(predictionCategories)


# Convert the training samples to a matrix
predictionMatrix <- predictionCategories[, !is.na(match(names(predictionCategories), str_remove_all(listVars$modelName, "\`")))] %>% as.matrix()
dim(predictionMatrix)

predictionScores <- predictionMatrix %*% scorecardVector
predictionEstimates <- allMatrix %*% estimateVector


```


```{r}

loansTest %>%
  dplyr::select(sub_grade) %>%
  cbind(predictionScores %>% as_tibble()) %>%
  dplyr::rename(Score = V1) %>%
  group_by(sub_grade) %>%
  summarise(Mean = mean(Score))

loansTest %>%
  dplyr::select(sub_grade) %>%
  cbind(outcomeEstimates %>% as_tibble()) %>%
  dplyr::rename(Estimate = V1) %>%
  group_by(sub_grade) %>%
  summarise(Mean = mean(Score))


loansTest %>%
  dplyr::select(sub_grade) %>%
  cbind(predictionScores %>% as_tibble()) %>%
  dplyr::rename(Score = V1) %>%
  dplyr::sample_n(50000) %>%
  ggplot(aes(sub_grade, Score)) +
  geom_point(alpha = 0.1, col = "blue")

loansTest %>%
  dplyr::select(sub_grade) %>%
  cbind(outcomeEstimates %>% as_tibble()) %>%
  dplyr::rename(Estimate = V1) %>%
  dplyr::sample_n(50000) %>%
  ggplot(aes(sub_grade, Estimate)) +
  geom_point(alpha = 0.1, col = "blue")

```




# OLD STUFF


```{r}
listVars <- listVars %>% 
  dplyr::mutate(scoreCard = round(ScoreFactor * Estimate + ScoreOffset)) %>% 

  # Remove the intercept row
  dplyr::filter(modelName != "(Intercept)")


```





Inspect why some values have NA

```{r}
#SGLMtrainNAs <- local({
  # 59 variables!
  listNAs <- listVars %>% 
  dplyr::filter(is.na(Estimate)) %>% 
  dplyr::select(modelName) %>% 
  dplyr::mutate(modelName = stringr::str_remove_all(modelName, "\`")) 
  
  nrow(listNAs)
  
  # Start at 2 to remove `loanID`
  loanSample <- allCategories[, 2:length(names(allCategories))] %>%
    cbind(loansTraining$isGoodLoan) %>%
    dplyr::rename(isGoodLoan = "loansTraining$isGoodLoan") %>%
    dplyr::mutate(isGoodLoan = if_else(isGoodLoan, 1, 0)) %>%
    as.data.frame()


  # Remove any categories uniformaly constant (only 0).
  # This is important when working on an extract of the dataset for variables that are seldom used
  # (e.g. customers from particular states)
  loanSample <- loanSample[, colSums(loanSample) != 0]
  ncol(loanSample)
  
  
  # About 8min. to complete training dataset
  {
    startTime <- proc.time()
    SGLMtrainNAs <-
      speedglm::speedglm(
        isGoodLoan ~ .,
        data = loanSample,
        family = binomial(),
        k = 0, 
        sparse = TRUE
      )
    cat(proc.time() - startTime)
  }
  
  return(SGLMtrainNAs)
})

summary(SGLMtrainNAs)

```
```{r}
# List of model variables
modelNames <- attr(SGLMtrain$coefficients, "names") %>%
  tibble::enframe() %>%
  dplyr::rename(modelName = "value")

# Create scorecard weight
listVars <- summary(SGLMtrain)$coefficients %>%
  dplyr::as_tibble() %>%
  cbind(modelNames) %>%
  dplyr::filter(!is.na(Estimate)) %>% 
  dplyr::rename(zValue = "z value") %>% 
  dplyr::filter(abs(zValue) >= 1.6)

# Score card formula: score = coefficient x Factor + Offset
#
# Factor: P points <=> T times more likely than not to defaults
# Factor= P / log(T)
#
# Offset is cosmetic to only have positive value (no negative scores)

ScoreFactor <- 100 / log(5)
ScoreOffset <- -floor(min(ScoreFactor * listVarsMax$Estimate) / 10) * 10

listVars <- listVars %>% 
  dplyr::mutate(scoreCard = round(ScoreFactor * Estimate + ScoreOffset)) %>% 

  # Remove the intercept row
  dplyr::filter(modelName != "(Intercept)")
```



```{r}
# List of variables with a significance value higher than 2
listVars <- listVars %>%
  dplyr::filter(!is.na(zValue) & abs(zValue) >= 2)


```






## Model training

```{r}
detach("package:smbinning", unload = TRUE, force = TRUE)
# devtools::install_github("Emmanuel-R8/SMBinning")
devtools::install_local(path = "~/Development/R/DVPT-PACKAGES/Score_modeling_binning/smbinning", force = TRUE)
library(smbinning)

loansBinning <- loans005 %>%
  dplyr::sample_n(250) %>%
  dplyr::mutate(home_ownership = as_factor(home_ownership)) %>%
  dplyr::select(-loanID) %>%
  as.data.frame()

smbinning.logitrank(
  df = loansBinning,
  y = "isGoodLoan",
  chr = as_vector(bestBins[, 1])
)
```





```{r}
## https://blog.revolutionanalytics.com/2015/03/r-package-smbinning-optimal-binning-for-scoring-modeling.html
data("smbsimdf1")
resultSmbinning <- smbinning(smbsimdf1, y = "fgood", x = "tob", p = 0.05)

resultSmbinning$ivtable
```
```{r}
# Relevant plots (2x2 Page)
par(mfrow = c(2, 2))

boxplot(
  smbsimdf1$tob ~ smbsimdf1$fgood,
  horizontal = T,
  frame = F,
  col = "lightgray",
  main = "Distribution"
)

mtext("Time on Books (Months)", 3)

smbinning.plot(resultSmbinning, option = "dist", sub = "Time on Books (Months)")
smbinning.plot(resultSmbinning, option = "badrate", sub = "Time on Books (Months)")
smbinning.plot(resultSmbinning, option = "WoE", sub = "Time on Books (Months)")
```

## Try all variables

```{r}
listIV <- readRDS("datasets/listIV100.rds")
listBins <- readRDS("datasets/listBins100.rds")

allBins <- listBins %>%
  arrange(desc(IV)) %>%
  filter(!(variable %in% c(
    "loanID", "term", "int_rate", "creditMargin", "loan_status",
    "grade", "sub_grade", "grade_num", "sub_grade_num",
    "emp_length", "home_ownership", "monthDefault",
    "principal_loss_pct", "creditMargin", "monthDefault",
    "isGoodLoan")))

# Variable will contain all the categories for all the variables.
# WARNING: 3.3 GB dataframe
allCategories <- loansTraining[,"loanID"]

for (index in 1:length(allBins$variable)) {

  binned <- categoriseFromWoE(df = loansTraining,
                              varName =  allBins$variable[index],
                              woeTable = allBins$WoE[[index]])

  allCategories <- cbind(allCategories, binned)
}

allCategories <- allCategories[, 2:length(names(allCategories))] %>%
  cbind(loansTraining$isGoodLoan) %>%
  dplyr::rename(isGoodLoan = "loansTraining$isGoodLoan") %>%
  dplyr::mutate(isGoodLoan = if_else(isGoodLoan, 1, 0)) %>%
  as.data.frame()
ncol(allCategories)

# Remove any categories uniformaly constant (only 0)
# This is important when working on an extract of the dataset for variables that are seldom used (e.g. customers from particular states)
allCategories <- allCategories[, colSums(allCategories) != 0]
ncol(allCategories)

# About 8min. to complete training dataset
{
  startTime <- proc.time()
  trainSGLMmax <- speedglm::speedglm(isGoodLoan ~ ., data = allCategories, family = binomial(), intercept = TRUE)
  cat(proc.time() - startTime)
}


summary(trainSGLMmax)
# saveRDS(trainSGLMmax, "datasets/trainSGLMmax.rds")


# List of model variables
modelNames <- attr(trainSGLMmax$coefficients, "names") %>%
  tibble::enframe() %>%
  dplyr::rename(modelName = "value")

```
