# Stochastic Gradient Descent


The diagram \@ref(fig:scikit-map)^[Source: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html] shows a useful decision tree.


```{r scikit-map, fig.cap="Scikit Learn algorithm cheat-sheet",output.width="70%",cache=FALSE}
knitr::include_graphics("images/scikit-learn-mlmap.png", auto_pdf = TRUE)
```

Following the disappointing results of the previous section, we will explore a simpler model, but iteratively trained on a wider set of random samples.

## Description of the model

Input - Layer - Single Q - Distribution



## Gradient descent

### Generalities

Gradient descent is a generic numerical optimisation algorithm to iteratively converge towards a (sometimes local) minumum of a given function. It is extensively used in statistical learning to minimise error functions.  

In the case of a simple linear regression model, the model training error $J$ (the __cost function__) as a function of $\theta$ is:

$$J_{(\theta)} = \frac{1}{2} \sum_{i=1}^{N}{\epsilon(y_i, \theta X_i)}$$
where the model parameters are denoted $\theta_i$, $X_i \in \mathbb{R^n}$ are the predictors, $Y_i \in \mathbb{R^n}$ are the responses and $\epsilon$ is a distance function. Typically, $\epsilon$ will be the Manhattan error or the Euclidian norm ($A = (a_1, \cdots , a_n), B = (b_1, \cdots , b_n)$).

**Manhattan**: $\epsilon(A, B) = \sum_{i=1}^n|a_i - b_i|$)$

**Euclidian norm**: $\epsilon(A, B) = \sqrt{\sum_{i=1}^n \left( a_i - b_i \right) ^2}$

The gradient descent algorithm uses the gradient of the error function, $\nabla J_{(\theta)}$, defined as:

$$
\nabla J_{(\theta)} = \left( \frac{\partial J}{\partial \theta_{0}},\frac{\partial J}{\partial \theta_{1}}, \cdots, \frac{\partial J}{\partial \theta_{p}} \right)
$$

And in the case of linear regression is in a matrix form that can be computed efficiently:

$$
\nabla J_{(\theta)} =  \left( y^{T} - \theta X^{T} \right) X
$$

The gradient decent algorithm finds parameters in the following manner iterating over the training samples:

While $|| \alpha \nabla J_{(\theta)} || > \eta$, $\theta :=  \theta - \alpha \nabla J_{(\theta)}$

In practice, the cost function will add a penalty term to regularise the model parameters (see below).


### Cost function as a function of the $Q$ parameter

Looking back at the distribution of the NPV between the -1 and about 1.5, it is multimodal and looks like the sum of 4 log-normal distributions with modes centered on about  


$$
PDF(x) = \frac{1}{x \sigma \sqrt{2 \pi}} exp{\left( -\frac{1}2 \left( \frac{ \log(x) - \mu }{\sigma} \right)^2 \right)}
$$

$$
\text{mode} = m = e^{\mu - \sigma^2} \text{, therefore: } \mu = log(\text{mode}) + \sigma^2
$$

$$
PDF(x) = \sqrt{\frac{e}{2 \pi}} \frac{1}{x \sigma}  exp{\left( -\frac{1}2 \left( \frac{\log(\frac{x}{m}) - \sigma^2}  {\sigma} \right)^2 \right)}
$$

We will center the distribution on the mode, therefore:

$$
PDF(x) = \sqrt{\frac{e}{2 \pi}} \frac{1}{\left( x - m \right) \sigma}  exp{ \left( -\frac{1}2 \left( \frac{\log(\frac{x - m}{m}) - \sigma^2}  {\sigma} \right)^2 \right) }
$$

The distribution's tail is towards positive infinity. For the symmetric result, we would replace $\left( x - m \right)$ by $-\left( x - m \right)$.


If we use 4 log-normal distributions, the cost function is:

$$
\operatorname{J}\left( x,Q\right) =-{{\left[ x-\left( \alpha_1 \operatorname{PDF_1}\left( x,Q\right) + \alpha_2 \operatorname{PDF_2}\left( x,Q\right) + \alpha_2 \operatorname{PDF_3}\left( x,Q\right) + \alpha_4 \operatorname{PDF_4}\left( x,Q\right) \right) \right] }^2}
$$

To optimise the shape of the total multi-modal distribution, we will assume that each $\alpha$, $m$ and $\sigma$ is a linear function of $Q$. The derivative $\frac{\partial{\operatorname{J}}}{\partial{Q}}$ is^[This was actually generated using Maxima (code in appendix) which allows for quicker iterations.]:


$$
\begin{array}{ccc}
\frac{\partial{\operatorname{J}}}{\partial{Q}} \left(x, Q \right) = - \sqrt{\frac{2}{\pi}} x
& - & \sqrt{\frac{2}{\pi}} \frac{\alpha_1}{\sigma_1 \left(-x+m_1 \right)} {e^{-\frac{1}{2} \left( \frac{ \log{\left( \frac{-x+m_1}{ m_1} \right)} + \sigma_1^2}{\sigma_1} \right) ^2}} \\
& - & \frac{1}{\sqrt{2 \pi}} \frac{\alpha_2}{\sigma_2 \left(-x+m_2 \right)} {e^{-\frac{1}{2} \left( \frac{ \log{\left( \frac{-x+m_2}{ m_2} \right)} + \sigma_2^2}{\sigma_2} \right) ^2}} \\
& - & \sqrt{\frac{2}{\pi}} \frac{\alpha_3}{\sigma_3 \left(-x+m_3 \right)} {e^{-\frac{1}{2} \left( \frac{ \log{\left( \frac{-x+m_3}{ m_3} \right)} + \sigma_3^2}{\sigma_3} \right) ^2}} \\
& - & \sqrt{\frac{2}{\pi}} \frac{\alpha_4}{\sigma_4 \left( x-m_4 \right)} {e^{-\frac{1}{2} \left( \frac{ \log{\left( \frac{ x-m_4}{ m_4} \right)} + \sigma_4^2}{\sigma_4} \right) ^2}}
\end{array}
$$







## Stochastic Gradient Descent

With realistic datasets, gradient descent can experience slow convergence because (1) each iteration requires calculation of the gradient for every single training example, and (2) since each individual sample is potentially very different from another, the calculated gradient may not be optimal. In such case, the gradient descent can be done using a batch of several training samples and use the average of the cost function (__batch gradient descent__). This addresses those two sources of inefficiency.

This method however still requires iterating over the entire dataset. We can instead iterate over batched of random training samples drawn from the entire dataset, instead of being drawn sequentially. This is the __stochastic gradient descent__.

Aside from the choice of the initial choice of samples, and the averaging of the cost function, the update of $\theta$ remains identical.



#### Cost function regularisation and derivative

Regularisation is a way to decrease the complexity of models by narrowing the range that parameters can take. It has long been established that it assists the stability and robustness of learning algorithms. See Chapter 13 of [@shalev2014understanding].

Our regularised cost function is written:

**[TODO]**
$$ 
J_{P,Q} = \sum_{(i, j) \in \Omega} {\left (  r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right )^2} + \frac{\lambda}{2} \left (  \sum_{i,k}{p_{i,k}^2} +  \sum_{j,k}{q_{j,k}^2} \right ) 
$$

The gradient descent algorithm seeks to minimise the $J_{(\theta)}$ cost function by step-wise update of each model parameter $x$ as follows:

$$
\theta_{t+1}^i \leftarrow \theta_{t}^i - \alpha \frac{\partial J_{(\theta)}}{\partial \theta_i} 
$$

The parameters are the matrix coefficients $p_{i,k}$ $q_{j,k}$. $\alpha$ is the learning parameter that needs to be adjusted.


#### Cost function partial derivatives

The partial derivatives of the cost function is:



$$
\frac{\partial J_{P,Q}}{\partial x} = \frac{\partial}{\partial x} \left ( \sum_{(i, j) \in \Omega} {\left ( r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right )^2} + \frac{\lambda}{2} \left ( \sum_{i,k}{p_{i,k}^2} + \sum_{j,k}{q_{j,k}^2} \right ) \right )
$$

$$
\frac{\partial J_{P,Q}}{\partial x} = \sum_{(i, j) \in \Omega} { 2 \frac{\partial r_{i,j} - \sum_{k=1}^{N_{features}} {p_{i,k} q_{j,k}}} {\partial x} \left (  r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right )  } + \frac{\lambda}{2} \left ( \sum_{i,k}{2 \frac{\partial p_{i,k}}{\partial x} p_{i,k}} + \sum_{j,k}{2 \frac{\partial p_{i,k}}{\partial x} q_{j,k}} \right ) 
$$

We note that $r_{i,j}$ are constants

$$
\frac{\partial J_{P,Q}}{\partial x} = 2 \sum_{(i, j) \in \Omega} \sum_{k=1}^{N_{features}} \left ( { \frac{\partial - {p_{i,k} q_{j,k}}}{\partial x} \left (  r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right ) }  \right ) + \lambda \left ( \sum_{i,k}{\frac{\partial p_{i,k}}{\partial x} p_{i,k}} + \sum_{j,k}{\frac{\partial p_{i,k}}{\partial x} q_{j,k}} \right ) 
$$



If $x$ is a coefficient of $P$ (resp. $Q$), say $p_{a,b}$ (resp. $q_{a,b}$), all partial derivatives will be nil unless for $(i,j) = (a,b)$. 

Therefore:


$$ 
\frac{\partial J_{P,Q}}{\partial p_{a,b}} = -2 \sum_{(i, j) \in \Omega} { q_{j,b} \left (  r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right )} + \lambda p_{a,b} 
$$

and,

$$
\frac{\partial J_{P,Q}}{\partial q_{a,b}} = -2 \sum_{(i, j) \in \Omega} { p_{i,b} \left (  r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right ) } + \lambda q_{a,b} 
$$

Since $\epsilon{i, j} = r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}}$ is the rating prediction error, this becomes:


$$ 
\frac{\partial J_{P,Q}}{\partial p_{a,b}} = -2 \sum_{(i, j) \in \Omega} { q_{j,b} \epsilon_{i,j}} + \lambda p_{a,b} 
$$

and,

$$
\frac{\partial J_{P,Q}}{\partial q_{a,b}} = -2 \sum_{(i, j) \in \Omega} { p_{i,b} \epsilon_{i,j} } + \lambda q_{a,b} 
$$

We note that the recent implementations of autodifferentiation algorithm (key to deep learning implementations) would avoid making those calculations by hand. We did not attempt to use the R `madness` package ^[https://github.com/shabbychef/madness].


### Stochastic Gradient Descent (SGD)

The size of the datasets is prohibitive to do those calculations across the entire training set. 

Instead, we will repeatedly update the model parameters on small random samples of the training set. 

Chapter 14 of [@shalev2014understanding] gives an extensive introduction to various SGD algorithms.

We implemented a simple version of the algorithm and present the code in more detail.



```{r MOVIELENS,eval=FALSE,echo=TRUE}
# The algorithm is implemented from scratch and relies on nothing but the `Tidyverse` libraries.
library(tidyverse)

# The quality of the training and predictions is measured by the _root mean squared error_
# (RMSE), for which we define a few helper functions (the global variables are defined
# later):
rmse_training <- function() {
  prediction_Z <- rowSums(LF_Model$P[tri_train$userN,] *
                            LF_Model$Q[tri_train$movieN,])
  prediction <- prediction_Z * r_sd + r_m
  sqrt(sum((tri_train$rating - prediction) ^ 2 / nSamples))
}

rmse_validation <- function() {
  prediction_Z <- rowSums(LF_Model$P[tri_test$userN,] *
                            LF_Model$Q[tri_test$movieN,])
  prediction <- prediction_Z * r_sd + r_m
  sqrt(sum((tri_test$rating - prediction) ^ 2) / nTest)
}

sum_square <- function(v) {
  return (sqrt(sum(v ^ 2) / nrow(v)))
}

# The key function updates the model coefficients. Its inputs are:
#
# + a list that contains the $P$ an $Q$ matrices, the training RMSE of those matrices, and
# a logical value indicating whether this RMSE is worse than what it was before the update
# (i.e. did the update diverge).
#
# + a `batch_size` that defines the number of samples to be drawn from the training set. A
# normal gradient descent would use the full training set; by default we only use 10,000
# samples out of 10 million (one tenth of a percent).
#
# + The cost regularisation `lambda` and gradient descent learning parameter `alpha`.
#
# + A number of `times` to run the descent before recalculating the RMSE and exiting the
# function (calculating the RMSE is computationally expensive).
#
#
# The training set used is less rich than the original set. As discussed, it only uses the
# rating (more exactly on the z_score of the rating). Genres, timestamps,... are
# discarded.


# Iterate gradient descent
stochastic_grad_descent <- function(model,
                                    times = 1,
                                    batch_size = 10000,
                                    lambda = 0.1,
                                    alpha = 0.01,
                                    verbose = TRUE) {
  # Run the descent `times` times.
  for (i in 1:times) {
    # Extract a sample of size `batch_size` from the training set.
    spl <- sample(1:nSamples, size = batch_size, replace = FALSE)
    spl_training_values <- tri_train[spl,]
    
    
    # Take a subset of `P` and `Q` matching the users and
    # movies in the training sample.
    spl_P <- model$P[spl_training_values$userN,]
    spl_Q <- model$Q[spl_training_values$movieN,]
    
    # rowSums returns the cross-product for a given user and movie.
    # err is the term inside brackets in the partial derivatives
    # calculation above.
    err <- spl_training_values$rating_z - rowSums(spl_P * spl_Q)
    
    # Partial derivatives wrt p and q
    delta_P <- -err * spl_Q + lambda * spl_P
    delta_Q <- -err * spl_P + lambda * spl_Q
    
    model$P[spl_training_values$userN,]  <- spl_P - alpha * delta_P
    model$Q[spl_training_values$movieN,] <- spl_Q - alpha * delta_Q
    
  }
  
  # RMSE against the training set
  error <- sqrt(sum((
    tri_train$rating_z - rowSums(model$P[tri_train$userN,] *
                                   model$Q[tri_train$movieN,])
  ) ^ 2)
  / nSamples)
  
  # Compares to RMSE before update
  model$WORSE_RMSE <- (model$RMSE < error)
  model$RMSE <- error
  
  # Print some information to keep track of success
  if (verbose) {
    cat(
      "  # features=",
      ncol(model$P),
      "  J=",
      nSamples * error ^ 2 +
        lambda / 2 * (sum(model$P ^ 2) + sum(model$Q ^ 2)),
      "  Z-scores RMSE=",
      model$RMSE,
      "\n"
    )
    flush.console()
  }
  
  return(model)
}


rm(list_results)
list_results <- tibble(
  "alpha" = numeric(),
  "lambda" = numeric(),
  "nFeatures" = numeric(),
  "rmse_training_z_score" = numeric(),
  "rmse_training" = numeric(),
  "rmse_validation" = numeric()
)

# The main training loop runs as follows:
#
# + We start with 3 features.
#
# + The model is updated in batches of 100 updates. This is done up to 250 times. At each
# time, if the model starts diverging, the learning parameter ($\alpha$) is reduced.
#
# + Once the 250 times have passed, or if $\alpha$ has become incredibly small, or if the
# RMSE doesn't really improve anymoe (by less than 1 millionth), we add another features
# and start again.

initial_alpha <- 0.1
for (n in 1:100) {
  # Current number of features
  number_features <- ncol(LF_Model$P)
  
  # lambda = 0.01 for 25 features, i.e. for about 2,000,000 parameters.
  # We keep lambda proportional to the number of features
  lambda <- 0.1 * (nUsers + nMovies) * number_features / 2000000
  
  alpha <- initial_alpha
  
  cat(
    "CURRENT FEATURES: ",
    number_features,
    "---- Pre-training validation RMSE = ",
    rmse_validation(),
    "\n"
  )
  
  list_results <- list_results %>% add_row(
    alpha = alpha,
    lambda = lambda,
    nFeatures = number_features,
    rmse_training_z_score = LF_Model$RMSE,
    rmse_training = rmse_training(),
    rmse_validation = rmse_validation()
  )
  
  for (i in 1:250) {
    pre_RMSE <- LF_Model$RMSE
    LF_Model <- stochastic_grad_descent(
      model = LF_Model,
      times = 100,
      batch_size = 1000 * number_features,
      alpha = alpha,
      lambda = lambda
    )
    
    list_results <- list_results %>% add_row(
      alpha = alpha,
      lambda = lambda,
      nFeatures = number_features,
      rmse_training_z_score = LF_Model$RMSE,
      rmse_training = rmse_training(),
      rmse_validation = rmse_validation()
    )
    
    if (LF_Model$WORSE_RMSE) {
      alpha <- alpha / 2
      cat("Decreasing gradient parameter to: ", alpha, "\n")
    }
    
    if (initial_alpha / alpha > 1000 |
        abs((LF_Model$RMSE - pre_RMSE) / pre_RMSE) < 1e-6) {
      break()
    }
  }
  
  
  # RMSE against validation set:
  rmse_validation_post <- rmse_validation()
  cat(
    "CURRENT FEATURES: ",
    number_features,
    "---- POST-training validation RMSE = ",
    rmse_validation_post,
    "\n"
  )
  
  # if (number_features == 12){
  #   break()
  # }
  
  
  # Add k features
  k_features <- 1
  LF_Model$P <- cbind(LF_Model$P,
                      matrix(
                        rnorm(
                          nrow(LF_Model$P) * k_features,
                          mean = 0,
                          sd = sd(LF_Model$P) / 100
                        ),
                        nrow = nrow(LF_Model$P),
                        ncol = k_features
                      ))
  
  LF_Model$Q <- cbind(LF_Model$Q,
                      matrix(
                        rnorm(
                          nrow(LF_Model$Q) * k_features,
                          mean = 0,
                          sd = sd(LF_Model$Q) / 100
                        ),
                        nrow = nrow(LF_Model$Q),
                        ncol = k_features
                      ))
  
}

saveRDS(list_results, "datasets/LRMF_results.rds")

```

