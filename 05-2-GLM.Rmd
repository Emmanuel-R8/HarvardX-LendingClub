## Logistic Regression

In this section, we fit a linear regression model using the fully binned dataset.


We tested several R linear regression packages. `glm` crashed on even small extracts of the dataset. `glmnet` returns errors that were not understandable nor documented on the internet. We settled on the `speedglm` package which is designed to handled very large datatsets. Note that it includes has a `select()` function which would shadow or conflict with `dplyr::select()`, so it is only used fully qualified to avoid any collision.




<!------------------------------------------------------------------------------------>
<!-- THIS IS A COPY OF PREVIOUS TO RELOAD DATASETS                                  -->

```{r 05-02-load-dataset,message=FALSE,echo=FALSE}
# Cleanup and reload

# Training set to speed up analysis
if (!exists("loansTraining")) {
  loansTraining <- readRDS("datasets/LoansTraining.rds")
}

# Take the allBins dataset (without `loanID`)
# Add the desired response

```


```{r 05-02-cleanup-load-dataset,message=FALSE,echo=FALSE}
if (!exists("allFactorsAsBins")) {
  allFactorsAsBins <- readRDS("datasets/allBins100.rds")
}

loanSampleBins <-
  allFactorsAsBins %>%
  select(-loanID) %>%
  cbind(loansTraining$isGoodLoan) %>%
  rename(isGoodLoan = "loansTraining$isGoodLoan") %>%
  mutate(isGoodLoan = if_else(isGoodLoan, 1, 0)) %>%
  as.data.frame()

rm(allFactorsAsBins)


if (!exists("allFactorsAsCharacteristics")) {
  allFactorsAsCharacteristics <-
    readRDS("datasets/allCharacteristics100.rds")
}

loanSampleCharacteristics <-
  allFactorsAsCharacteristics %>%
  select(-loanID) %>%
  cbind(loansTraining$isGoodLoan) %>%
  rename(isGoodLoan = "loansTraining$isGoodLoan") %>%
  mutate(isGoodLoan = if_else(isGoodLoan, 1, 0)) %>%
  as.data.frame()

rm(allFactorsAsCharacteristics)
```


```{r message=FALSE,echo=FALSE}
rm(loansTraining)
```

<!-- END OF COPY                                                                     -->
<!------------------------------------------------------------------------------------->


```{r message=FALSE,echo=FALSE}
ncol(loanSampleBins)
ncol(loanSampleCharacteristics)
```


### Remove identical bins

All linear regression algorithms require the variables to not have any linear relationship. Any colinearity prevents fitting a coefficient to such variables.

Although the original dataset may not contain such variables, binning variables into discrete factors will create such situations. The original variables were not identical, but when split into categories, some bins can become identical. For example, if a loan application does not provide income information (`income=NA` is TRUE for that loan), no debt-to-income ratio can be calculated (`dti=NA` will also be true for that loan). The `income` and `dti` variables were not colinear, but some of their bins can be. 


To identify such bins, we use a "trick" to speed up comparison: we run a hash digest on each column and spot identical hashes. 

$$\text{ }$$
```{r 05-02-rm-useless-bins,message=FALSE,echo=TRUE}
namesCharacteristics <- names(loanSampleCharacteristics)
namesBins <- names(loanSampleBins)

# Starting list of names, calculate a hash value for each column with that name and
# identify duplicates.
duplicateNames <- names(
                    loanSampleBins[
                      duplicated(
                        lapply(loanSampleBins, digest::digest))])
```
$$\text{ }$$

We also identify any empty bin (this has only been useful when working on extremely small extracts of the original dataset).

$$\text{ }$$
```{r message=FALSE,echo=TRUE}
# Remove any categories uniformaly constant (only 0)
#
# This is important when working on a small extract of the dataset for variables that are
# seldom used (e.g. customers from certain states).
zeroColumnsNames <- loanSampleBins[, colSums(loanSampleBins) == 0] %>% names()

```
$$\text{ }$$



Fitting the model will be done in four consecutive steps. We start with the binned dataset, removing variables with IV under 2%. All variables are one-hot encoded. 

  1. We fit a model on the binned dataset.
  
  2. Next, we refit after removing any variable identified as colinear (i.e. NA coefficient).
  
  3. Next, we refit after removing any variable whose significance is not at least p-value > 95%.
  
Finally, we will select a model.


### First model - complete dataset


$$\text{ }$$
```{r 05-02-first-training-B,echo=TRUE,message=FALSE}
# About 700 sec wall-time to complete training dataset
# The dataset is the sample less duplicate, less zero-ed columns
{
  doMC::registerDoMC(cores = 1) # Too many processes push over 32GB
  startTime <- proc.time()
  
  loansData <- loanSampleBins
  
  SGLM_B_train <- speedglm::speedglm(isGoodLoan ~ .,
                                     data = loansData,
                                     family = binomial())
  
  doMC::registerDoMC(cores = NULL)
  cat(proc.time() - startTime, "\n")
}
```
$$\text{ }$$


```{r echo=FALSE,message=FALSE}
saveRDS(SGLM_B_train, "datasets/SGLM_B_train.rds")
```


There are a number of NA coefficients. The first 10 are:

$$\text{ }$$
```{r 05-02-first-training-B-NA,echo=TRUE,message=FALSE}
speedglm:::summary.speedglm(SGLM_B_train)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Bin name") %>% 
  filter(is.na(Estimate)) %>% 
  slice(1:10) %>% 
  select("Bin name") %>% 
  kable(caption = "First model. Example of NA coefficients.", digits = 3) %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position")

```
$$\text{ }$$


The most significant bins are:

$$\text{ }$$
```{r 05-02-first-training-B-best10,echo=TRUE,message=FALSE}
speedglm:::summary.speedglm(SGLM_B_train)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Bin_name") %>% 
  rename(z_value = "z value") %>% 
  arrange(desc(z_value)) %>% 
  select(Bin_name, Estimate, z_value) %>% 
  slice(1:15) %>%
  kable(caption = "First training. 15 best bins", digits = 3) %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position")

```
$$\text{ }$$


We store the list of bins identified as colinear for later use. 

$$\text{ }$$
```{r  05-02-first-training-B-vars,echo=TRUE,message=FALSE}
NAsFirstTraining <-
  
  # Take the results (just the estimated coefficients) from the model
  tibble(
    name = rownames(summary(SGLM_B_train)$coefficient),
    estimate = summary(SGLM_B_train)$coefficient$Estimate
  ) %>%
  
  # Reformat the names to be the same as in the bins
  mutate(name = stringr::str_remove_all(name, "\`")) %>% 
  
  # Make sure that repsonse is not deleted by mistake and list all NAs
  filter(name != "isGoodLoan" & is.na(estimate)) 

NAsFirstTraining <- NAsFirstTraining$name

```
$$\text{ }$$

### Second training - fitted model less colinear bins


For the second training, we use the complete dataset with duplicate and colinear bins removed. 

$$\text{ }$$
```{r 05-02-second-training-B,echo=TRUE,message=FALSE}
# Start the model training again
{
  startTime <- proc.time()
  doMC::registerDoMC(cores = 2) # Too many processes push over 32GB if more than 2
  
  loansData <- loanSampleBins %>%
    select(-one_of( c(duplicateNames, 
                      zeroColumnsNames, 
                      NAsFirstTraining)))
  
  SGLM_B_retrain <- speedglm::speedglm(isGoodLoan ~ .,
                                       data = loansData,
                                       family = binomial())
  
  doMC::registerDoMC(cores = NULL)
  cat(proc.time() - startTime)
}
```
$$\text{ }$$

```{r echo=FALSE,message=FALSE}
saveRDS(SGLM_B_retrain, "datasets/SGLM_B_retrain.rds")
```



The most significant bins are:

$$\text{ }$$
```{r 05-02-second-training-B-best10,echo=TRUE,message=FALSE}

speedglm:::summary.speedglm(SGLM_B_retrain)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Bin_name") %>% 
  rename(z_value = "z value") %>% 
  arrange(desc(z_value)) %>% 
  select(Bin_name, Estimate, z_value) %>% 
  slice(1:15) %>%
  kable(caption = "Second training. 15 best bins", digits = 3) %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position")

```
$$\text{ }$$


We can now select any bins whose significance value is under 2 $\sigma$. 

$$\text{ }$$
```{r 05-02-second-training-B-vars,echo=TRUE,message=FALSE}
NAsSecondTraining <-
  tibble(
    name = rownames(summary(SGLM_B_retrain)$coefficient),
    estimate = summary(SGLM_B_retrain)$coefficient$Estimate, 
    zValue = abs(summary(SGLM_B_retrain)$coefficient$"z value")
  ) %>%
  
  mutate(name = stringr::str_remove_all(name, "\`")) %>% 
  filter(name != "isGoodLoan") %>%
  
  # Remove stray NAs or anything less than 2 sigmas
  filter(is.na(estimate) | zValue < 2)

NAsSecondTraining <- NAsSecondTraining$name

```
$$\text{ }$$


### Third training - second fitted model less non-significant bins




For the third training, we use the complete dataset with duplicate, colinear and non-significant bins removed. 

$$\text{ }$$
```{r 05-02-third-training-B,echo=TRUE,message=FALSE}
# Start the model training again. About 350 sec.
{
  
  startTime <- proc.time()
  doMC::registerDoMC(cores = 2)
  
  loansData <- loanSampleBins %>%
    select(-one_of( c(
      duplicateNames,
      zeroColumnsNames,
      NAsFirstTraining,
      NAsSecondTraining
    )))
  
  SGLM_B_reretrain <- speedglm::speedglm(isGoodLoan ~ .,
                                         data = loansData,
                                         family = binomial())
  
  doMC::registerDoMC(cores = NULL)
  cat(proc.time() - startTime)
}
```
$$\text{ }$$

```{r echo=FALSE,message=FALSE}
saveRDS(SGLM_B_reretrain, "datasets/SGLM_B_reretrain.rds")
```



The most significant bins are:

$$\text{ }$$
```{r 05-02-third-training-B-best10,echo=TRUE,message=FALSE}

speedglm:::summary.speedglm(SGLM_B_reretrain)$coefficients %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Bin_name") %>% 
  rename(z_value = "z value") %>% 
  arrange(desc(z_value)) %>% 
  select(Bin_name, Estimate, z_value) %>% 
  slice(1:15) %>%
  kable(caption = "Third training: 15 best bins", digits = 3) %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position")

```
$$\text{ }$$


$$\text{ }$$
```{r 05-02-third-training-B-vars,echo=FALSE,message=FALSE}
NAsThirdTraining <-
  tibble(
    name     = rownames(summary(SGLM_B_reretrain)$coefficient),
    estimate = summary(SGLM_B_reretrain)$coefficient$Estimate, 
    zValue   = abs(summary(SGLM_B_reretrain)$coefficient$"z value")
  ) %>%
  mutate(name = stringr::str_remove_all(name, "\`")) %>% 
  filter(name != "isGoodLoan") %>%
  
  # Remove stray NAs (Shouldn't be any) or less than 2 sigmas
  filter(is.na(estimate) | zValue < 2)

NAsThirdTraining <- NAsThirdTraining$name

```
$$\text{ }$$


Table \@ref(tab:05-02-training-criteria) shows the Akaike Information Criterion and Log-likelihood for each of the 3 training results. Two points to note:

  + The results did not change from first to second training. This is normal since we only removed non-informative categorical bins for which the regression could not determine a coefficient (i.e. colinearity with other variables);

  + Removing seemingly non-significant bins makes the model worse (at least by those measures). This would suggest that individual bins should not be considered but only the entire variable to which they belong.


$$\text{ }$$
```{r 05-02-training-criteria,echo=FALSE}
tibble(
  "Criteria" = c("AIC", "Log likelihood"),
  "Training #1" = c(
    speedglm:::summary.speedglm(SGLM_B_train)$aic,
    speedglm:::summary.speedglm(SGLM_B_train)$logLik
  ),
  
  "Training #2" = c(
    speedglm:::summary.speedglm(SGLM_B_retrain)$aic,
    speedglm:::summary.speedglm(SGLM_B_retrain)$logLik
  ),
  
  "Training #3" = c(
    speedglm:::summary.speedglm(SGLM_B_reretrain)$aic,
    speedglm:::summary.speedglm(SGLM_B_reretrain)$logLik
  )
) %>%
  
  kable(caption = "Training AIC and Log-likelihood", digits = 3) %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position")

```
$$\text{ }$$


In the next section we will only work with the second model since it appears to be the best.


```{r echo=FALSE,message=FALSE}
rm(loansData)
```

