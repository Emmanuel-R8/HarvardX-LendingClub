---
title: "R Notebook"
output: html_notebook
---

# Logistic Regression

## Logistic regression using the `SpeedGLM` package


> WARNING: `speedglm` has a `select()` function which shadows `dplyr::select()`. From there on, functions are fully qualified to avoid any collision.


```{r}

# Cleanup and reload

rm(list = ls())
library(tidyverse)

loansTraining <- readRDS("datasets/LoansTraining.rds")

# Training results
SGLMtrain     <- readRDS("datasets/SGLMtrain.rds")
SGLMretrain   <- readRDS("datasets/SGLMretrain.rds")
SGLMreretrain   <- readRDS("datasets/SGLMreretrain.rds")


# WARNING: 3.3 GB dataframe 
# Take the allBins dataset (without `loanID`)
# Add the desired response
loanSample <- 
  readRDS("datasets/allBins100.rds") %>% 
  select(-loanID) %>%
  cbind(loansTraining$isGoodLoan) %>%
  rename(isGoodLoan = "loansTraining$isGoodLoan") %>%
  mutate(isGoodLoan = if_else(isGoodLoan, 1, 0)) %>%
  as.data.frame()

rm(loansTraining)

gc(full = TRUE)
ncol(loanSample)

```



### Remove identical bins

```{r}

# Starting list of names
namesBins <- names(loanSample)

# Running GLM would give a number of NAs if some columns are linearly dependant (in our case actually equal).
# A "trick" to identify them is to run a hash digest on each column and spot identical hashes
# The original variables were not identical, but when split into categories, they can become identical. 
# For example, if no income is provided (income=NA category), no debt-to-income ratio can be calculated (dti=NA).
duplicateNames <- loanSample[duplicated(lapply(loanSample, digest::digest))] %>% names()
duplicateNames

# Remove any categories uniformaly constant (only 0)
# This is important when working on a small extract of the dataset for variables that are seldom used (e.g. customers
# from certain states).
zeroColumnsNames <- loanSample[, colSums(loanSample) == 0] %>% names()

```


### First training on variables previously selected on their Information Value


We use `speedglm` being quick. Note that alternatives were also tried: `glm` crashed on even small extracts of the dataset. `glmnet` returns errors that were not understandable or documented on the internet. 


```{r}
gc(full = TRUE)


# About 700 sec wall-time to complete training dataset
# The dataset is the sample less duplicate, less zero-ed columns
{
  doMC::registerDoMC(cores = 1) # Too many processes push over 32GB
  startTime <- proc.time()
  
  loansTrainingBins <- loanSample %>%
    select(-one_of( c(duplicateNames, 
                      zeroColumnsNames)))
  
  SGLMtrain <- speedglm::speedglm(isGoodLoan ~ .,
                                  data = loanTrainingBins,
                                  family = binomial())
  
  doMC::registerDoMC(cores = NULL)
  cat(proc.time() - startTime, "\n")
}

summary(SGLMtrain)

# saveRDS(SGLMtrain, "datasets/SGLMtrain.rds")


```



```{r}
# Does the model throw NAs? They are produced not only for identical variables, but also co-linear
# combinations.
# Let us select those variables:

NAsFirstTraining <-
  
  # Take the results (just the estimated coefficients) from the model
  tibble(
    name = rownames(summary(SGLMtrain)$coefficient),
    estimate = summary(SGLMtrain)$coefficient$Estimate
  ) %>%
  
  # Reformat the names to be the same as in the bins
  mutate(name = stringr::str_remove_all(name, "\`")) %>% 
  
  # Make sure that repsonse is not deleted by mistake and list all NAs
  filter(name != "isGoodLoan" & is.na(estimate)) 

NAsFirstTraining <- NAsFirstTraining$name

```

### Second training

```{r}
gc(full = TRUE)

# Start the model training again
{
  startTime <- proc.time()
  doMC::registerDoMC(cores = 2) # Too many processes push over 32GB
  
  loansTrainingBins <- loanSample %>%
    select(-one_of( c(duplicateNames, 
                      zeroColumnsNames, 
                      NAsFirstTraining)))
  
  SGLMretrain <- speedglm::speedglm(isGoodLoan ~ .,
                                  data = loanTrainingBins,
                                  family = binomial())
  
  doMC::registerDoMC(cores = NULL)
  cat(proc.time() - startTime)
}

summary(SGLMretrain)
# saveRDS(SGLMretrain, "datasets/SGLMretrain.rds")

```


```{r}
# Just in case, new NAs popped up 
# Actual variables after excluding intercept coefficient

NAsSecondTraining <-
  tibble(
    name = rownames(summary(SGLMretrain)$coefficient),
    estimate = summary(SGLMretrain)$coefficient$Estimate, 
    zValue = abs(summary(SGLMretrain)$coefficient$"z value")
  ) %>%
  mutate(name = stringr::str_remove_all(name, "\`")) %>% 
  filter(name != "isGoodLoan") %>%
  
  # Remove stray NAs or anything less than 2 sigmas
  filter(is.na(estimate) | zValue < 2)

NAsSecondTraining <- NAsSecondTraining$name


```

### Third training


```{r}
gc(full = TRUE)

# Start the model training again. About 350 sec.
{
  startTime <- proc.time()
  doMC::registerDoMC(cores = 2)
  
  loansTrainingBins <- loanSample %>%
    select(-one_of( c(duplicateNames, 
                      zeroColumnsNames, 
                      NAsFirstTraining, 
                      NAsSecondTraining)))
  
  SGLMreretrain <- speedglm::speedglm(isGoodLoan ~ .,
                                  data = loanTrainingBins,
                                  family = binomial())
  
  doMC::registerDoMC(cores = NULL)
  cat(proc.time() - startTime)
}

summary(SGLMreretrain)

```


```{r}
NAsThirdTraining <-
  tibble(
    name     = rownames(summary(SGLMreretrain)$coefficient),
    estimate = summary(SGLMreretrain)$coefficient$Estimate, 
    zValue   = abs(summary(SGLMreretrain)$coefficient$"z value")
  ) %>%
  mutate(name = stringr::str_remove_all(name, "\`")) %>% 
  filter(name != "isGoodLoan") %>%
  
  # Remove stray NAs (Shouldn't be any) or less than 2 sigmas
  filter(is.na(estimate) | zValue < 2)

NAsThirdTraining <- NAsThirdTraining$name

```



## Balanced training

### Train on balanced subset of good loans

Remove the same duplicate and NAs as before

```{r}
loanSample <- readRDS("datasets/LoanSample.rds")

# Add the desired response
loanSample <- 
  allBins[, 2:length(names(allBins))] %>%
  cbind(loansTraining$isGoodLoan) %>%
  rename(isGoodLoan = "loansTraining$isGoodLoan") %>%
  mutate(isGoodLoan = if_else(isGoodLoan, 1, 0)) %>%
  select(-one_of(duplicateNames)) %>% 
  as.data.frame()



NAsFirstTraining <-
  tibble(
    name = rownames(summary(SGLMtrain)$coefficient),
    estimate = summary(SGLMtrain)$coefficient$Estimate
  ) %>%
  mutate(name = stringr::str_remove_all(name, "\`")) %>% 
  filter(name != "isGoodLoan") %>%
  filter(!is.na(estimate))


# We will remove those variables from the dataset.
loanSample %>% 
  select(-one_of(NAsFirstTraining$name))


ncol(loanSample)

```

How many bad loans


```{r}
loanSample %>% 
  group_by(isGoodLoan) %>% 
  summarise(n= n())

```

Create balanced training sets with equal numbers of each. 

```{r}
loanSampleBad <- 
  loanSample %>% 
  filter(isGoodLoan == 0)

nBad <- nrow(loanSampleBad)

kSubSet <- 2

listGLM <- list()

for (i in 1:kSubSet) {
  loanSampleBalanced <-
    
    # From the sample of loans
    loanSample %>%
    
    # From the good loans
    filter(isGoodLoan == 1) %>%
    
    # Select the same number as bad
    sample_n(nBad) %>%
    
    # To create a new balanced dataset
    rbind(loanSampleBad)

  # Train on the balanced set - About 30 sec per training
  startTime <- proc.time()
  doMC::registerDoMC(cores = 3)
  modelBalanced <-
    speedglm::speedglm(
      isGoodLoan ~ .,
      data = loanSampleBalanced,
      family = binomial())
  
  doMC::registerDoMC(cores = NULL)
  cat(proc.time() - startTime, "\n")
    
  listGLM <- c(listGLM, list(modelBalanced))
}


```

```{r}
summary(listGLM[[2]])
summary(listGLM[[2]])$coefficients$Estimate
```



```{r}
gc(full = TRUE)

# Start the model training again. About 350 sec.
{
  startTime <- proc.time()
  doMC::registerDoMC(cores = 3)
  
  SGLMreretrain <-
    speedglm::speedglm(
      isGoodLoan ~ .,
      data = loanSample,
      family = binomial(),
      intercept = TRUE
    )
  
  doMC::registerDoMC(cores = NULL)
  cat(proc.time() - startTime)
}

summary(SGLMreretrain)

# saveRDS(SGLMreretrain, "datasets/SGLMreretrain.rds")

```


# Model performance


### Final list of variables

```{r}

# Model to use
GLModel <-  SGLMtrain

# List of model variables
modelNames <- attr(GLModel$coefficients, "names") %>%
  tibble::enframe(x = .) %>%
  rename(modelName = "value")


# and their coefficients in the model
listVars <- summary(GLModel)$coefficients %>%
  as_tibble() %>%
  cbind(modelNames) %>%
  rename(zValue = "z value",
         pValue = "Pr(>|z|)",
         stdErr = "Std. Error")

```


### Scoring

Scoring expresses the coefficients that were estimated during the logistic regression into points on a scale. The conversion is done using  three parameters that are chosen somewhat arbitrarily.

+ the number of points increase / decrease that would reflect halving / doubling the odds of defaulting;

+ an _anchoring_ score reflecting a particular odd.

For our purpose, we will choose 1,000 points being equivalent to 1 in a 100 to default, i.e. 2,000 points <=> odds = 99. We will also choose 50 to reflect _times 2_ change in odds. 


```{r}
# Prob of doubling = 100 points
ScoreFactor <- 100 / log(2)

# 2,000 points is 20:1 odds of default
ScoreOffset <- 2000 - ScoreFactor * log(20)  

# Score at the intercept  
Intercept <- summary(SGLMreretrain)$coefficients["(Intercept)", "Estimate"]

ScorePerVariable <- (ScoreFactor * Intercept  + ScoreOffset) / nrow(modelNames)

```




Using the model is a simple matrix multiplication: $\text{Loan matrix} \times \text{Scorecard weights}$

## Training set

```{r}
# Reload the right datasets
allBins <- readRDS("datasets/allBins100.rds")
loansTraining <- readRDS("datasets/LoansTraining.rds")
loansTest     <- readRDS("datasets/LoansTest.rds")
gc(full = TRUE)
```


```{r}
# Remove every variable that is not in the list of variables in the model then convert into a matrix
allMatrix <- 
  allBins[, !is.na(match(names(allBins), 
                               str_remove_all(listVars$modelName, "\`")))] %>% 
  as.matrix()

# Add a column of 1s for the intercept
allMatrix <- cbind( as.vector(rep.int(x = 1, times = dim(allMatrix)[1])), allMatrix )
dim(allMatrix)

# Done with this dataset
rm(allBins)
gc(full = TRUE)
```


```{r}
estimateVector <- listVars$Estimate %>% as.matrix()

# Score per variable
estimateScore <- ScoreFactor * estimateVector + ScorePerVariable
estimateScore[is.na(estimateScore)] <- 0

dim(estimateVector)
dim(estimateScore)


# Any variable with NAs?
listVars$modelName[which(is.na(estimateVector))]

estimateVector[is.na(estimateVector)] <- 0
dim(estimateVector)

estimateCoefficients <- allMatrix %*% estimateVector 
estimateCoefficients <- 
  tibble::enframe(estimateCoefficients[,1]) %>% 
  mutate(p = 1 / (1 + exp(-value)), 
         oddsGood = if_else(is.infinite(p / (1 - p)), 1e10, p / (1 - p)), 
         score = oddsGood * ScoreFactor + ScoreOffset)
  

estimateScorecard <- allMatrix %*% estimateScore
```


# Histogram of the scores

```{r}
loansTraining %>% 
  cbind(estimateCoefficients) %>% 
  cbind(estimateScorecard) %>% 
  ggplot(aes(estimateScorecard)) +
  geom_histogram(bins = 25, col = "blue", fill = "lightblue")

loansTraining %>% 
  cbind(estimateCoefficients) %>% 
  ggplot(aes(oddsGood)) +
  geom_histogram(bins = 25, col = "blue", fill = "lightblue") +
  scale_x_log10()


loansTraining %>% 
  cbind(estimateCoefficients) %>% 
  ggplot(aes(score)) +
  geom_histogram(bins = 25, col = "blue", fill = "lightblue") +
  scale_x_log10()
```


```{r}
median(estimateScorecard)

```




```{r}
loansTraining %>%
  select(sub_grade) %>%
  cbind(estimateCoefficients) %>%
  group_by(sub_grade) %>%
  summarise(Mean = mean(oddsGood))


loansTraining %>%
  # Add the newly calculated estimates to the training set
  cbind(estimateCoefficients %>% as_tibble()) %>% 

  # Calculate a mean estimate by sub rating
  group_by(sub_grade) %>%
  mutate(Mean = mean(p)) %>% 
  ungroup() %>% 

  sample_n(100000) %>%
  ggplot(aes(sub_grade, p, Mean)) +
  geom_violin(aes(sub_grade, p), alpha = 0.1, col = "blue", adjust = 0.5) +
  geom_point(aes(sub_grade, Mean), col = "red")

loansTraining %>%
  # Add the newly calculated estimates to the training set
  cbind(estimateCoefficients %>% as_tibble()) %>% 

  # Calculate a mean estimate by sub rating
  group_by(sub_grade) %>%
  mutate(Mean = mean(oddsGood)) %>% 
  ungroup() %>% 

  sample_n(100000) %>%
  ggplot(aes(sub_grade, oddsGood, Mean)) +
  geom_violin(aes(sub_grade, oddsGood), alpha = 0.1, col = "blue", adjust = 0.5) +
  geom_point(aes(sub_grade, Mean), col = "red") + 
  scale_y_log10()


loansTraining %>%
  # Add the newly calculated scorecards to the training set
  cbind(estimateScorecard %>% as_tibble()) %>%
  rename(ScoreCard = V1) %>%

  # Calculate a mean estimate by sub rating
  group_by(sub_grade) %>%
  mutate(Mean = mean(ScoreCard)) %>% 
  ungroup() %>% 

  sample_n(100000) %>%
  ggplot(aes(sub_grade, ScoreCard, Mean)) +
  geom_violin(aes(sub_grade, ScoreCard), alpha = 0.1, col = "blue", adjust = 0.5) +
  geom_point(aes(sub_grade, Mean), col = "red") 

```


## Test set

```{r}
# Prepare the full test set
predictionCategories <- loansTest[,"loanID"]

for (index in 1:length(bestBins$variable)) {

  binned <- 
    binner::categoriseFromWoE(df = loansTest,
                              varName =  bestBins$variable[index],
                              woeTable = bestBins$WoE[[index]])

  predictionCategories <- cbind(predictionCategories, binned)
}
dim(predictionCategories)

# Retain only the relevant scorecard categories
predictionMatrix <-   
  predictionCategories[, !is.na(match(names(predictionCategories), 
                               str_remove_all(listVars$modelName, "\`")))] %>% 
  as.matrix()

predictionMatrix <- cbind( as.vector(rep.int(x = 1, times = dim(predictionMatrix)[1])), predictionMatrix )
dim(predictionMatrix)


predictionCoefficients <- predictionMatrix %*% estimateVector
predictionCoefficients <- 
  tibble::enframe(predictionCoefficients[,1]) %>% 
  mutate(p = 1 / (1 + exp(-value)), 
                oddsGood = if_else(is.infinite(p / (1 - p)), 1e10, p / (1 - p)))
  
predictionScorecard <- predictionMatrix %*% estimateScore



```

```{r}
loansTest %>% 
  cbind(predictionCoefficients) %>% 
  cbind(predictionScorecard) %>% 
  mutate(scoreBand = floor(predictionScorecard / 20) * 20) %>% 
  filter(predictionScorecard > 0) %>% 
  group_by(scoreBand) %>% 
  summarise(Count = n()) %>% 
  ggplot(aes(scoreBand, Count)) +
  geom_col(col = "blue")

median(predictionScorecard)

```

Same downward dynamics as training set

```{r}
loansTest %>%
  select(sub_grade) %>%
  cbind(predictionCoefficients) %>%
  group_by(sub_grade) %>%
  summarise(Mean = mean(oddsGood))


loansTest %>%
  # Add the newly calculated estimates to the training set
  cbind(predictionCoefficients %>% as_tibble()) %>% 

  # Calculate a mean estimate by sub rating
  group_by(sub_grade) %>%
  mutate(Mean = mean(p)) %>% 
  ungroup() %>% 

  sample_n(100000) %>%
  ggplot(aes(sub_grade, p, Mean)) +
  geom_violin(aes(sub_grade, p), alpha = 0.1, col = "blue", adjust = 0.5) +
  geom_point(aes(sub_grade, Mean), col = "red")

loansTest %>%
  # Add the newly calculated estimates to the training set
  cbind(predictionCoefficients %>% as_tibble()) %>% 

  # Calculate a mean estimate by sub rating
  group_by(sub_grade) %>%
  mutate(Mean = mean(oddsGood)) %>% 
  ungroup() %>% 

  sample_n(100000) %>%
  ggplot(aes(sub_grade, oddsGood, Mean)) +
  geom_violin(aes(sub_grade, oddsGood), alpha = 0.1, col = "blue", adjust = 0.5) +
  geom_point(aes(sub_grade, Mean), col = "red") + 
  scale_y_log10()


loansTest %>%
  # Add the newly calculated scorecards to the training set
  cbind(predictionScorecard %>% as_tibble()) %>%
  rename(ScoreCard = V1) %>%

  # Calculate a mean estimate by sub rating
  group_by(sub_grade) %>%
  mutate(Mean = mean(ScoreCard)) %>% 
  ungroup() %>% 

  sample_n(100000) %>%
  ggplot(aes(sub_grade, ScoreCard, Mean)) +
  geom_violin(aes(sub_grade, ScoreCard), alpha = 0.1, col = "blue", adjust = 0.5) +
  geom_point(aes(sub_grade, Mean), col = "red") 


```


## ROC Curve

```{r}
ROCRPrediction <- ROCR::prediction(predictionScorecard, loansTest$isGoodLoan)
ROCRPerformance <- ROCR::performance(ROCRPrediction, "tpr", "fpr")

ROCRPlot <- 
  tibble(x = ROCRPerformance@x.values[[1]], y = ROCRPerformance@y.values[[1]]) 


ROCRPlot %>% ggplot(aes(x, y)) +
  geom_point()

stats::ks.test(x = ROCRPlot$x, y = ROCRPlot$y)
```

```{r}
str(ROCRPerformance)
ROCRPerformance@x.values
```


## Bin test set by prediction

```{r}
probEstimates <- if_else(predictionEstimates < 0, 0, predictionEstimates)

summary(probEstimates / (1+probEstimates))


```
