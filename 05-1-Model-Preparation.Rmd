# Logistic Regression Model and Credit Scorecard


```{r 05-01-change-printoutoption,echo=FALSE}

# knitr global settings - By default, the final document will not include source code unless
# expressly stated.
knitr::opts_chunk$set(
  # Chunks
  eval = TRUE,
  cache = TRUE,
  echo = TRUE,
  message = FALSE,
  warning = FALSE)


```


At the outset, the dataset presents a number of challenges:

+ There is a mix of continuous and categorical data.

+ The number of observations is very large. 

+ The possible number of predictors is large (partially due to one-hot encoding of categorical values).

+ As shown in the previous section, credit margins have changed over time. This is clearly related to the wider US economic environment. Financial hardship is a key driver for some of the loans. Availability of disposable income is important to assess the ability to repay. Therefore, the cost of living, which varies from state to state, seems relevant. 


[TODO: Bias/Complexity trade-off]
[TODO: AoC? or AuC?]


## Introduction

### Split

The dataset was randomly split into training and validation sets (80/20 ratio).

We initiated the exploration of potential models with Principal Component Analysis, Linear Regression, Extreme Boosting and Random Forest. No model could be trained on the full set. We therefore came to limit the training set on a random sample of 0.1% (1 thousandth) of the initial full set.

Any model will require training in batch (e.g. stochastic gradient descent) or online. Our untested intuition is that online methods are not adapted to an unbalanced dataset: the number of defaults/write-offs is fairly low for high quality ratings. However, the dataset is evidently a time series which points to online training. We will not explore that line of investigation, although exploring that tension would be an interesting subject.

### The modeling task

Two step process:
- Given dataset (+/- rating?) find a score between -5 and +5
- Given a score between -5 and +5 given the right parameters for the modes
- Given a density curve extrapolate the required margin

- Make money or not?



## Logistic Regression

__Logistic models__ (also called __logit model__) are used to model binary events. Examples would be passing or failing an exam,  a newborn being a boy or a girl, a voter choosing a particular political party, or -- relevant to us -- a borrower defaulting or not on a loan. If the binary variable is modeled as a 0/1 outcome, the model will yield a value between 0 and 1 which can be used as a probabilty. 

We are interested in using a number of variables (being continuous and/or categorical) to model the binary response. A natural model is a linear combination of the variables. Since the predicted value would be continuous and not be bounded by 0/1, the outcome is transformed. A commonly used transformation of the logodds (logarithm of the odds given a particular probability) $\log \left( \frac{p}{1 - p} \right)$. This expression has a few advantages: it converts any value (between $- \infty$ and $+\infty$ produced by the linear regression), and it is symmetrical around $x = 0$ and $y = 1/2$. That is, using the odds instead of the probability avoids infinity; it behaves identically when p approaches 0 or 1 (this would not be the case using p). The reciprocal of the logodds is $p = \frac{1}{1 - e^{-x}}$. 

For a number of $X_i$ variables, the model to fit is then:

$$p =  \frac{1}{1 - e^{-\sum_i{\alpha_i X_i}}}$$

The commonly used method to evaluate the creditworthiness of a borrower is to create scorecards whereby particular characteristics are segmented into intervals and attributed discrete scores. In plain English, a continuous variable (say the age of the applicant) is segmented into intervals (e.g. 0-18 year-olds, 18-26 year-olds,...), and then given a score. Those different segments become categorical variables. The task of the model is to:
 
 + identify the best way to segment a continuous variable to maximise the information value of the different segments (called bins): intuitively empty or quasi-empty  bins (either no or few applicants in the bin) are not informative; bins for which the response is completely random are not predictive, whereas a bin where the response always has the same response is informative (i.e. anybody with income of 0 and 10 dolars per year will default, anybody with a salary of $1 million per month will repay).
 
 + use a generalised linear model using the new categorical variables; 
 
 + transforms the linear coefficients estimated for each category into numerical scores.




> WARNING: To run this model, you will need at least 32GB of memory (real plus virtual/swap space) 

### Data preparation

>
> WARNING: Execute `CleanLoad.Rmd` first
>


```{r 05-01-m1-clean-load,echo=FALSE,child="CleanLoad.Rmd"}
```
```{r}
# AFTER EXECUTION OF `CleanLoad.Rmd`

# Variables not used
rm(lending_club, loans)
```



```{r 05-01-vars-inout}
# select the variables that might be used to create the training+test set 
modelVarsIn <- c(LC_variable[LC_variable$inModel == TRUE, "variable_name"])$variable_name
modelVarsIn <- c(modelVarsIn, 
                 "grade_num", "sub_grade_num", 
                 "principal_loss_pct", "creditMargin", "monthDefault")

# Make sure that some variables are NOT in included in the final training set
modelVarsOut <- c("grade_num", "sub_grade_num", 
                  "principal_loss_pct", "creditMargin", "monthDefault", 
                  "zip_code")
```


```{r 05-01-loan-predictors}
## ###############################################################################################
##
## Prepare a dataset with ONLY the predictors NOT removing NA's
## 
loansPredictors <-
  loansWorkingSet %>%

  # Keep the chosen predictors
  # Use tidyselect::one_of() to avoid errors if column does not exist
  select(tidyselect::one_of(modelVarsIn)) %>%

  ##
  ## Dates to numeric, in 'decimal' years since 2000
  ##
  mutate_at(c("issue_d", "earliest_cr_line"), function(d) {
    return(year(d) - 2000 + (month(d) - 1) / 12)
  }) %>%

  ## Add polynomials of the dates to model the time-trend shape
  mutate(
    issue_d2 = issue_d^2,
    issue_d3 = issue_d^3,
    earliest_cr_line2 = earliest_cr_line^2,
    earliest_cr_line3 = earliest_cr_line^3
  ) %>%

  ## Create a logical flag TRUE for non-defaulted (good) loans
  mutate(isGoodLoan = (principal_loss_pct < 0.001)) %>%

  select(-tidyselect::one_of(modelVarsOut))



## ###############################################################################################
##
## Create training / test sets 80%/20%
##
proportionTraining <- 0.8
set.seed(42)

nSamples <- nrow(loansPredictors)

sampleTraining <- sample(1:nSamples, floor(nSamples * proportionTraining), replace = FALSE)
loansTraining <- loansPredictors %>% slice(sampleTraining)
loansTest <- loansPredictors %>% slice(-sampleTraining)

# Subsets of the training set
set.seed(42)
nSamplesTraining <- nrow(loansTraining)

# 1%
sample01 <- sample(1:nSamplesTraining, floor(nSamplesTraining * 0.01), replace = FALSE)
loans01 <- loansTraining %>% slice(sample01)

# 20%
sample20 <- sample(1:nSamplesTraining, floor(nSamplesTraining * 0.20), replace = FALSE)
loans20 <- loansTraining %>% slice(sample20)
```

```{r 05-01-cleanup}
# Not used later on
rm(loansWorkingSet, loansPredictors, LoansNPV, LoansIRR, LoansMargin, RATES, RATES3Y, RATES5Y)
gc(full = TRUE)
```


## Binning and Weight of Evidence

This subsection owes a lot influence to the `smbinning` package source code from which we reimplemented some aspects using the `tidyverse` __style__  (the original source code uses SQL statements to access dataframes), and the `Information` package vignette ^[ [](https://cran.r-project.org/web/packages/Information/vignettes/Information-vignette.html)].


To identify the various bins (and number and location)


Letâ€™s say that we have a binary dependent variable Y and a set of predictive variables X1,,Xp. As mentioned above, Y can capture a wide range of outcomes, such as defaulting on a loan, clicking on an ad, or terminating a subscription.

WOE and IV play two distinct roles when analyzing data:

    WOE describes the relationship between a predictive variable and a binary target variable.
    IV measures the strength of that relationship.




```{r 05-01-reinstall-binner}

# Ensure that `binner` is here and available
if ("package:binner" %in% search()) { 
  detach("package:binner", unload = TRUE, force = TRUE) 
}

if (!("binner" %in% installed.packages()[,1])) {
  devtools::install_local("~/Development/R/DVPT-PACKAGES/Score_modeling_binning/binner", 
                          force = TRUE, 
                          quiet = TRUE)
  # devtools::install_github("Emmanuel-R8/SMBinning")
}

library(binner)
```

### Loop through all variables

```{r 05-01-create-list-bins}
loansBinning <- loansTraining %>%
  mutate(
    home_ownership = as_factor(home_ownership),
    emp_length = as_factor(emp_length),
    grade = as_factor(grade)
  )

informationValues <- tibble(variable = names(loansBinning), IV = -1.0)

listBins <-
  tibble(
    variable = "",
    type = "",
    IV = 0.0,
    WoE = list(),
    .rows = 0
  )
```


```{r 05-01-create-all-bins,message=FALSE}
# About 500 sec wall-time
startTime <- proc.time()
for (n in informationValues$variable) {
  cat("Variable: ", n)

  # We don't test the response with itself
  if (n %in% c("isGoodLoan")) {
    cat("\n")

  } else {
    if (class(loansBinning[[1, n]]) == "factor") {
      cat(" is a factor, ")
      result <- WoETableCategorical(
        df = loansBinning,
        x = n,
        y = "isGoodLoan",
        maxCategories = 100)

    } else {
      cat(" is numeric, ")
      result <- WoETableContinuous(df = loansBinning,
                                   x = n,
                                   y = "isGoodLoan",
                                   p = 0.05)
    }

    tryCatch({
      if (is.na(result)) {
        cat("Variable skipped.\n")
        add_row(
          listBins,
          variable = n,
          type = NA,
          IV = NA
        )
      } else {
        cat("     IV : ", result$IV, "\n")

        listBins <- listBins %>%
          add_row(
            variable = n,
            type = result$type,
            IV = result$IV,
            WoE = list(result$table)
          )
        
        informationValues[n] <- result$IV
        
        }
    },
    finally = {})
  }
}
proc.time() - startTime

saveRDS(informationValues, "datasets/informationValues100.rds")
saveRDS(listBins, "datasets/listBins100.rds")

```

### Select relevant variables

It is considered that information value below 2% is useless. 

```{r}
bestBins <- 
  listBins %>% 
  filter(IV >= 0.02)

```


```{r 05-01-create-bestbins-10best}
bestBins %>% 
  select(variable, IV) %>% 
  mutate(IV = round(IV, digits = 5)) %>% 
  arrange(desc(IV)) %>% 
  slice(1:10)
```


However, we should ignore the variables that would not be available at the time the credit scoring is performed. 


```{r 05-01-create-bestbins}
bestBins <- bestBins %>%
  filter(!(variable %in% c(
    "loanID", "term", "int_rate", "creditMargin", "loan_status",
    "grade", "sub_grade", "grade_num", "sub_grade_num",
    "emp_length", "home_ownership", "monthDefault",
    "principal_loss_pct", "creditMargin", "monthDefault",
    "isGoodLoan")))

saveRDS(bestBins, "datasets/bestBins100.rds")

```


The 10 best variables that will retain are in the following table. Interestingly, the square and cubic powers of the issue date are retained. 

```{r 05-01-create-bestbins-10worst}
bestBins %>% 
  select(variable, IV) %>% 
  mutate(IV = round(IV, digits = 5)) %>% 
  arrange(desc(IV)) %>% 
  slice(1:10)
```

And the 10 worst (but retained) variables are:

```{r 05-01-create-bestbins-10worst}
bestBins %>% 
  select(variable, IV) %>% 
  mutate(IV = round(IV, digits = 5)) %>% 
  arrange(IV) %>% 
  slice(1:10)
```





### Create data table with only binary variables (transform every bin to a 0/1 value)

For each variable, create new variables for each bin in the WoE table of that variable.

```{r 05-01-create-factored-datasets-loanID}
# Variable will contain all the best characteristics. Every continuous variable is reformatted
# into factors reflecting the appropriate bins

allFactorsAsCharacteristics <- loansTraining[,"loanID"]
allFactorsAsBins <- loansTraining[,"loanID"]
```


```{r 05-01-create-factored-datasets}
for (index in 1:nrow(bestBins)) {
#for (index in 1:27) {
  name <- bestBins$variable[index][[1]]
  
  cat("--------------------",
      index,
      "--",
      name, "\n")
  
  ltIndex <- which(names(loansTraining) == name)
  
  characteristic <- categoriseFromWoE(df = loansTraining[, ltIndex],
                                      varName =  name,
                                      woeTable = bestBins$WoE[index][[1]])
  
  bins <- categoriseFromWoE.Wide(df = loansTraining[, ltIndex],
                                 varName =  name,
                                 woeTable = bestBins$WoE[index][[1]])
  
  allFactorsAsCharacteristics <-
    allFactorsAsCharacteristics %>%
    cbind(characteristic)
  
  
  allFactorsAsBins <-
    allFactorsAsBins %>%
    cbind(bins)
  
}
```


```{r 05-01-create-factored-datasets-save}
saveRDS(allFactorsAsCharacteristics, "datasets/allCharacteristics100.rds") ; saveRDS(allFactorsAsBins, "datasets/allBins100.rds")

```



### Comparison of individual characteristics

>
> [TODO]
> Present data by order of IV
> Sort WoE buildup for key IV


+ re-train secures the lowest Akaike criterion and will be chosen.

+ On this test, the best variables are: SORT BY IV. Then WoE for the first few ones.



```{r}
loansBinning <- loans20 %>%
  mutate(
    home_ownership = as_factor(home_ownership),
    emp_length = as_factor(emp_length)
  )

best10 <- bestBins %>% 
  arrange(desc(IV)) %>% 
  slice(1:10)
```

#### dti

```{r}
plotBinWoE <- function(n = 1) {
  vName <- best10[n,]$variable
  
  if (best10[n,]$type == "numeric") {
    best10[n,]$WoE[[1]] %>%
      arrange(WoE) %>%
      ggplot(aes(Name, WoE)) +
      geom_col(col = "blue", fill = "lightblue") +
      theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
      ggtitle(vName)
  } else {
    best10[n,]$WoE[[1]] %>%
      arrange(WoE) %>%
      ggplot(aes(Name, WoE)) +
      geom_col(col = "blue", fill = "lightblue") +
      theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
      ggtitle(vName)
    
  }
  
}

```

```{r}
plotBinWoE(10)
```



#### Home ownership

```{r}
resultHome <- WoETable(
  df = loansBinning,
  x = "home_ownership",
  y = "isGoodLoan",
  maxCategories = 20
)

resultHome
resultHome$IV
```

#### Verification Status

```{r}
resultVerif <- WoETable(
  df = loansBinning,
  x = "verification_status",
  y = "isGoodLoan",
  maxCategories = 20
)

resultVerif
resultVerif$IV
```



#### Purpose

```{r}
resultHome <- WoETable(
  df = loansBinning,
  x = "purpose",
  y = "isGoodLoan",
  maxCategories = 20
)

resultHome
resultHome$IV
```


#### Sub grade

```{r}
resultSubgrade <- WoETable(
  df = loansBinning,
  x = "sub_grade",
  y = "isGoodLoan",
  maxCategories = 50
)

resultSubgrade
resultSubgrade$IV
```



#### Employment years

```{r}
resultEmp <- WoETable(
  df = loansBinning,
  x = "emp_length",
  y = "isGoodLoan",
  maxCategories = 20
)

resultEmp
resultEmp$IV
```



#### Bank card utilisation

```{r}
resultBC <- WoETable(
  df = loansBinning,
  x = "bc_util",
  y = "isGoodLoan",
  p = 0.05
)

resultBC
resultBC$IV
```

#### State

```{r}
resultState <- WoETable(
  df = loansBinning,
  x = "addr_state",
  y = "isGoodLoan",
  maxCategories = 100
)

resultState
resultState$IV
```


#### Annual income

```{r}
which(names(loansBinning) == "annual_inc_joint")

resultInc <- WoETable(
  df = loansBinning,
  x = "annual_inc_joint",
  y = "isGoodLoan",
  p = 0.05
)

resultHome
resultHome$iv
```

#### Loan amount

```{r}
resultAmnt <- WoETable(
  df = loansBinning,
  x = "loan_amnt",
  y = "isGoodLoan",
  p = 0.05
)

resultAmnt
resultAmnt$IV
```
