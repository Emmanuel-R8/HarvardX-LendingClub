<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Stochastic Gradient Descent | LendingClub Loans Pricing</title>
  <meta name="description" content="HarvardX - PH125.9x Data Science Capstone" />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Stochastic Gradient Descent | LendingClub Loans Pricing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="HarvardX - PH125.9x Data Science Capstone" />
  <meta name="github-repo" content="Emmanuel_R8/HarvardX-LendingClub" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Stochastic Gradient Descent | LendingClub Loans Pricing" />
  
  <meta name="twitter:description" content="HarvardX - PH125.9x Data Science Capstone" />
  

<meta name="author" content="Emmanuel Rialland - https://github.com/Emmanuel_R8" />


<meta name="date" content="2019-12-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="errands-and-post-mortem.html"/>
<link rel="next" href="appendix.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./introduction.html" target="blank>LendingClub Loans Pricing</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="internal-rate-of-return-credit-margins-and-net-present-values.html"><a href="internal-rate-of-return-credit-margins-and-net-present-values.html"><i class="fa fa-check"></i><b>1</b> Internal Rate of Return, Credit Margins and Net Present Values</a><ul>
<li class="chapter" data-level="1.1" data-path="internal-rate-of-return-credit-margins-and-net-present-values.html"><a href="internal-rate-of-return-credit-margins-and-net-present-values.html#important-warning"><i class="fa fa-check"></i><b>1.1</b> Important warning</a></li>
<li class="chapter" data-level="1.2" data-path="internal-rate-of-return-credit-margins-and-net-present-values.html"><a href="internal-rate-of-return-credit-margins-and-net-present-values.html#background"><i class="fa fa-check"></i><b>1.2</b> Background</a></li>
<li class="chapter" data-level="1.3" data-path="internal-rate-of-return-credit-margins-and-net-present-values.html"><a href="internal-rate-of-return-credit-margins-and-net-present-values.html#internal-rate-of-return"><i class="fa fa-check"></i><b>1.3</b> Internal Rate of Return</a></li>
<li class="chapter" data-level="1.4" data-path="internal-rate-of-return-credit-margins-and-net-present-values.html"><a href="internal-rate-of-return-credit-margins-and-net-present-values.html#dataset-calculation"><i class="fa fa-check"></i><b>1.4</b> Dataset calculation</a><ul>
<li class="chapter" data-level="1.4.1" data-path="internal-rate-of-return-credit-margins-and-net-present-values.html"><a href="internal-rate-of-return-credit-margins-and-net-present-values.html#irr"><i class="fa fa-check"></i><b>1.4.1</b> IRR</a></li>
<li class="chapter" data-level="1.4.2" data-path="internal-rate-of-return-credit-margins-and-net-present-values.html"><a href="internal-rate-of-return-credit-margins-and-net-present-values.html#credit-margins"><i class="fa fa-check"></i><b>1.4.2</b> Credit Margins</a></li>
<li class="chapter" data-level="1.4.3" data-path="internal-rate-of-return-credit-margins-and-net-present-values.html"><a href="internal-rate-of-return-credit-margins-and-net-present-values.html#npv"><i class="fa fa-check"></i><b>1.4.3</b> NPV</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="dataset.html"><a href="dataset.html"><i class="fa fa-check"></i><b>2</b> Dataset</a><ul>
<li class="chapter" data-level="2.1" data-path="dataset.html"><a href="dataset.html#preamble"><i class="fa fa-check"></i><b>2.1</b> Preamble</a></li>
<li class="chapter" data-level="2.2" data-path="dataset.html"><a href="dataset.html#general-presentation"><i class="fa fa-check"></i><b>2.2</b> General presentation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="dataset.html"><a href="dataset.html#business-volume"><i class="fa fa-check"></i><b>2.2.1</b> Business volume</a></li>
<li class="chapter" data-level="2.2.2" data-path="dataset.html"><a href="dataset.html#loan-lifecyle-and-status"><i class="fa fa-check"></i><b>2.2.2</b> Loan lifecyle and status</a></li>
<li class="chapter" data-level="2.2.3" data-path="dataset.html"><a href="dataset.html#loan-application"><i class="fa fa-check"></i><b>2.2.3</b> Loan application</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="dataset.html"><a href="dataset.html#rates"><i class="fa fa-check"></i><b>2.3</b> Rates</a><ul>
<li class="chapter" data-level="2.3.1" data-path="dataset.html"><a href="dataset.html#irr-and-required-credit-margins"><i class="fa fa-check"></i><b>2.3.1</b> IRR and required credit margins</a></li>
<li class="chapter" data-level="2.3.2" data-path="dataset.html"><a href="dataset.html#dataset-1"><i class="fa fa-check"></i><b>2.3.2</b> Dataset</a></li>
<li class="chapter" data-level="2.3.3" data-path="dataset.html"><a href="dataset.html#interest-rates"><i class="fa fa-check"></i><b>2.3.3</b> Interest rates</a></li>
<li class="chapter" data-level="2.3.4" data-path="dataset.html"><a href="dataset.html#purpose"><i class="fa fa-check"></i><b>2.3.4</b> Purpose</a></li>
<li class="chapter" data-level="2.3.5" data-path="dataset.html"><a href="dataset.html#payments"><i class="fa fa-check"></i><b>2.3.5</b> Payments</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="dataset.html"><a href="dataset.html#net-present-value"><i class="fa fa-check"></i><b>2.4</b> Net present value</a><ul>
<li class="chapter" data-level="2.4.1" data-path="dataset.html"><a href="dataset.html#average-npv-and-credit-margin-by-subgrade"><i class="fa fa-check"></i><b>2.4.1</b> Average NPV and credit margin by subgrade</a></li>
<li class="chapter" data-level="2.4.2" data-path="dataset.html"><a href="dataset.html#principal-losses"><i class="fa fa-check"></i><b>2.4.2</b> Principal losses</a></li>
<li class="chapter" data-level="2.4.3" data-path="dataset.html"><a href="dataset.html#distribution-of-principal-losses-by-rating"><i class="fa fa-check"></i><b>2.4.3</b> Distribution of principal losses by rating</a></li>
<li class="chapter" data-level="2.4.4" data-path="dataset.html"><a href="dataset.html#npv-distribution-by-rating"><i class="fa fa-check"></i><b>2.4.4</b> NPV distribution by rating</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="dataset.html"><a href="dataset.html#loan-decision"><i class="fa fa-check"></i><b>2.5</b> Loan decision</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html"><i class="fa fa-check"></i><b>3</b> Logistic Regression Model and Credit Scorecard</a><ul>
<li class="chapter" data-level="3.1" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#logistic-regression"><i class="fa fa-check"></i><b>3.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#data-preparation"><i class="fa fa-check"></i><b>3.2.1</b> Data preparation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#binning-and-weight-of-evidence"><i class="fa fa-check"></i><b>3.3</b> Binning and Weight of Evidence</a><ul>
<li class="chapter" data-level="3.3.1" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#background-1"><i class="fa fa-check"></i><b>3.3.1</b> Background</a></li>
<li class="chapter" data-level="3.3.2" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#binning"><i class="fa fa-check"></i><b>3.3.2</b> Binning</a></li>
<li class="chapter" data-level="3.3.3" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#woe-and-iv-definitions"><i class="fa fa-check"></i><b>3.3.3</b> WOE and IV definitions</a></li>
<li class="chapter" data-level="3.3.4" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#modeling"><i class="fa fa-check"></i><b>3.3.4</b> Modeling</a></li>
<li class="chapter" data-level="3.3.5" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#loop-through-all-variables"><i class="fa fa-check"></i><b>3.3.5</b> Loop through all variables</a></li>
<li class="chapter" data-level="3.3.6" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#select-relevant-variables"><i class="fa fa-check"></i><b>3.3.6</b> Select relevant variables</a></li>
<li class="chapter" data-level="3.3.7" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#create-data-table-with-one-hot-encoding"><i class="fa fa-check"></i><b>3.3.7</b> Create data table with one-hot encoding</a></li>
<li class="chapter" data-level="3.3.8" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#comparison-of-individual-characteristics"><i class="fa fa-check"></i><b>3.3.8</b> Comparison of individual characteristics</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#logistic-regression-1"><i class="fa fa-check"></i><b>3.4</b> Logistic Regression</a><ul>
<li class="chapter" data-level="3.4.1" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#remove-identical-bins"><i class="fa fa-check"></i><b>3.4.1</b> Remove identical bins</a></li>
<li class="chapter" data-level="3.4.2" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#first-model---complete-dataset"><i class="fa fa-check"></i><b>3.4.2</b> First model - complete dataset</a></li>
<li class="chapter" data-level="3.4.3" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#second-training---fitted-model-less-colinear-bins"><i class="fa fa-check"></i><b>3.4.3</b> Second training - fitted model less colinear bins</a></li>
<li class="chapter" data-level="3.4.4" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#third-training---second-fitted-model-less-non-significant-bins"><i class="fa fa-check"></i><b>3.4.4</b> Third training - second fitted model less non-significant bins</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#model-result"><i class="fa fa-check"></i><b>3.5</b> Model result</a><ul>
<li class="chapter" data-level="3.5.1" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#final-list-of-variables"><i class="fa fa-check"></i><b>3.5.1</b> Final list of variables</a></li>
<li class="chapter" data-level="3.5.2" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#scoring"><i class="fa fa-check"></i><b>3.5.2</b> Scoring</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#training-set"><i class="fa fa-check"></i><b>3.6</b> Training set</a><ul>
<li class="chapter" data-level="3.6.1" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#densities-of-the-training-results"><i class="fa fa-check"></i><b>3.6.1</b> Densities of the training results</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#test-set"><i class="fa fa-check"></i><b>3.7</b> Test set</a></li>
<li class="chapter" data-level="3.8" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#confusion-matrix"><i class="fa fa-check"></i><b>3.8</b> Confusion matrix</a></li>
<li class="chapter" data-level="3.9" data-path="logistic-regression-model-and-credit-scorecard.html"><a href="logistic-regression-model-and-credit-scorecard.html#roc-curve"><i class="fa fa-check"></i><b>3.9</b> ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>4</b> Conclusion</a></li>
<li class="chapter" data-level="5" data-path="errands-and-post-mortem.html"><a href="errands-and-post-mortem.html"><i class="fa fa-check"></i><b>5</b> Errands and post-mortem</a><ul>
<li class="chapter" data-level="5.1" data-path="errands-and-post-mortem.html"><a href="errands-and-post-mortem.html#geographical-data"><i class="fa fa-check"></i><b>5.1</b> Geographical data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="stochastic-gradient-descent.html"><a href="stochastic-gradient-descent.html"><i class="fa fa-check"></i><b>6</b> Stochastic Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="stochastic-gradient-descent.html"><a href="stochastic-gradient-descent.html#description-of-the-model"><i class="fa fa-check"></i><b>6.1</b> Description of the model</a></li>
<li class="chapter" data-level="6.2" data-path="stochastic-gradient-descent.html"><a href="stochastic-gradient-descent.html#gradient-descent"><i class="fa fa-check"></i><b>6.2</b> Gradient descent</a><ul>
<li class="chapter" data-level="6.2.1" data-path="stochastic-gradient-descent.html"><a href="stochastic-gradient-descent.html#generalities"><i class="fa fa-check"></i><b>6.2.1</b> Generalities</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="stochastic-gradient-descent.html"><a href="stochastic-gradient-descent.html#stochastic-gradient-descent-1"><i class="fa fa-check"></i><b>6.3</b> Stochastic Gradient Descent</a><ul>
<li class="chapter" data-level="6.3.1" data-path="stochastic-gradient-descent.html"><a href="stochastic-gradient-descent.html#stochastic-gradient-descent-sgd"><i class="fa fa-check"></i><b>6.3.1</b> Stochastic Gradient Descent (SGD)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="stochastic-gradient-descent.html"><a href="stochastic-gradient-descent.html#various-errands"><i class="fa fa-check"></i><b>6.4</b> Various errands</a><ul>
<li class="chapter" data-level="6.4.1" data-path="stochastic-gradient-descent.html"><a href="stochastic-gradient-descent.html#optimising-the-scorecard-or-rating-depending-on-the-npv-curve."><i class="fa fa-check"></i><b>6.4.1</b> Optimising the scorecard (or rating) depending on the NPV curve.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="6.5" data-path="appendix.html"><a href="appendix.html#sec:list-assumptions"><i class="fa fa-check"></i><b>6.5</b> List of assumptions / limitations regarding the dataset</a></li>
<li class="chapter" data-level="6.6" data-path="appendix.html"><a href="appendix.html#data-preparation-and-formatting"><i class="fa fa-check"></i><b>6.6</b> Data preparation and formatting</a><ul>
<li class="chapter" data-level="6.6.1" data-path="appendix.html"><a href="appendix.html#sec:lendingclub-dataset"><i class="fa fa-check"></i><b>6.6.1</b> LendinClub dataset</a></li>
<li class="chapter" data-level="6.6.2" data-path="appendix.html"><a href="appendix.html#zip-codes-and-fips-codes"><i class="fa fa-check"></i><b>6.6.2</b> Zip codes and FIPS codes</a></li>
<li class="chapter" data-level="6.6.3" data-path="appendix.html"><a href="appendix.html#market-interest-rates"><i class="fa fa-check"></i><b>6.6.3</b> Market interest rates</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="appendix.html"><a href="appendix.html#sec:list-model-variables"><i class="fa fa-check"></i><b>6.7</b> List of variables</a></li>
<li class="chapter" data-level="6.8" data-path="appendix.html"><a href="appendix.html#calculations-of-the-internal-rate-of-returns-and-month-to-default"><i class="fa fa-check"></i><b>6.8</b> Calculations of the internal rate of returns and month-to-default</a><ul>
<li class="chapter" data-level="6.8.1" data-path="appendix.html"><a href="appendix.html#r-code"><i class="fa fa-check"></i><b>6.8.1</b> R code</a></li>
<li class="chapter" data-level="6.8.2" data-path="appendix.html"><a href="appendix.html#julia-code"><i class="fa fa-check"></i><b>6.8.2</b> Julia code</a></li>
<li class="chapter" data-level="6.8.3" data-path="appendix.html"><a href="appendix.html#maxima-derivation-of-the-cost-function"><i class="fa fa-check"></i><b>6.8.3</b> Maxima derivation of the cost function</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="appendix.html"><a href="appendix.html#system-version"><i class="fa fa-check"></i><b>6.9</b> System version</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org/" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">LendingClub Loans Pricing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="stochastic-gradient-descent" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Stochastic Gradient Descent</h1>
<p>The diagram <a href="stochastic-gradient-descent.html#fig:scikit-map">6.1</a><a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a> shows a useful decision tree.</p>

<div class="sourceCode" id="cb89"><pre class="sourceCode numberSource r numberLines lineAnchors"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" href="#cb89-1" data-line-number="1">knitr<span class="op">::</span><span class="kw">include_graphics</span>(<span class="st">&quot;images/scikit-learn-mlmap.png&quot;</span>, <span class="dt">auto_pdf =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:scikit-map"></span>
<img src="images/scikit-learn-mlmap.png" alt="Scikit Learn algorithm cheat-sheet" width="70%" />
<p class="caption">
Figure 6.1: Scikit Learn algorithm cheat-sheet
</p>
</div>

<p>Following the disappointing results of the previous section, we will explore a simpler model, but iteratively trained on a wider set of random samples.</p>
<div id="description-of-the-model" class="section level2">
<h2><span class="header-section-number">6.1</span> Description of the model</h2>
</div>
<div id="gradient-descent" class="section level2">
<h2><span class="header-section-number">6.2</span> Gradient descent</h2>
<div id="generalities" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Generalities</h3>
<p>Gradient descent is a generic numerical optimisation algorithm to iteratively converge towards a (sometimes local) minumum of a given function. It is extensively used in statistical learning to minimise error functions.</p>
<p>In the case of a simple linear regression model, the model training error <span class="math inline">\(J\)</span> (the <em>cost function</em>) as a function of <span class="math inline">\(\theta\)</span> is:</p>
<p><span class="math display">\[J_{(\theta)} = \frac{1}{2} \sum_{i=1}^{N}{\epsilon(y_i, \theta X_i)}\]</span>
where the model parameters are denoted <span class="math inline">\(\theta_i\)</span>, <span class="math inline">\(X_i \in \mathbb{R^n}\)</span> are the predictors, <span class="math inline">\(Y_i \in \mathbb{R^n}\)</span> are the responses and <span class="math inline">\(\epsilon\)</span> is a distance function. Typically, <span class="math inline">\(\epsilon\)</span> will be the Manhattan error or the Euclidian norm (<span class="math inline">\(A = (a_1, \cdots , a_n), B = (b_1, \cdots , b_n)\)</span>).</p>
<p><strong>Manhattan</strong>: <span class="math inline">\(\epsilon(A, B) = \sum_{i=1}^n|a_i - b_i|\)</span>)$</p>
<p><strong>Euclidian norm</strong>: <span class="math inline">\(\epsilon(A, B) = \sqrt{\sum_{i=1}^n \left( a_i - b_i \right) ^2}\)</span></p>
<p>The gradient descent algorithm uses the gradient of the error function, <span class="math inline">\(\nabla J_{(\theta)}\)</span>, defined as:</p>
<p><span class="math display">\[
\nabla J_{(\theta)} = \left( \frac{\partial J}{\partial \theta_{0}},\frac{\partial J}{\partial \theta_{1}}, \cdots, \frac{\partial J}{\partial \theta_{p}} \right)
\]</span></p>
<p>And in the case of linear regression is in a matrix form that can be computed efficiently:</p>
<p><span class="math display">\[
\nabla J_{(\theta)} =  \left( y^{T} - \theta X^{T} \right) X
\]</span></p>
<p>The gradient decent algorithm finds parameters in the following manner iterating over the training samples:</p>
<p>While <span class="math inline">\(|| \alpha \nabla J_{(\theta)} || &gt; \eta\)</span>, <span class="math inline">\(\theta := \theta - \alpha \nabla J_{(\theta)}\)</span></p>
<p>In practice, the cost function will add a penalty term to regularise the model parameters (see below).</p>
</div>
</div>
<div id="stochastic-gradient-descent-1" class="section level2">
<h2><span class="header-section-number">6.3</span> Stochastic Gradient Descent</h2>
<p>With realistic datasets, gradient descent can experience slow convergence because (1) each iteration requires calculation of the gradient for every single training example, and (2) since each individual sample is potentially very different from another, the calculated gradient may not be optimal. In such case, the gradient descent can be done using a batch of several training samples and use the average of the cost function (<strong>batch gradient descent</strong>). This addresses those two sources of inefficiency.</p>
<p>This method however still requires iterating over the entire dataset. We can instead iterate over batched of random training samples drawn from the entire dataset, instead of being drawn sequentially. This is the <em>stochastic gradient descent</em>.</p>
<p>Aside from the choice of the initial choice of samples, and the averaging of the cost function, the update of <span class="math inline">\(\theta\)</span> remains identical.</p>
<div id="cost-function-regularisation-and-derivative" class="section level4">
<h4><span class="header-section-number">6.3.0.1</span> Cost function regularisation and derivative</h4>
<p>Regularisation is a way to decrease the complexity of models by narrowing the range that parameters can take. It has long been established that it assists the stability and robustness of learning algorithms. See Chapter 13 of <span class="citation">(Shalev-Shwartz and Ben-David <a href="#ref-shalev2014understanding">2014</a>)</span>.</p>
<p>Our regularised cost function is written:</p>
<p><strong>[TODO]</strong>
<span class="math display">\[ 
J_{P,Q} = \sum_{(i, j) \in \Omega} {\left (  r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right )^2} + \frac{\lambda}{2} \left (  \sum_{i,k}{p_{i,k}^2} +  \sum_{j,k}{q_{j,k}^2} \right ) 
\]</span></p>
<p>The gradient descent algorithm seeks to minimise the <span class="math inline">\(J_{(\theta)}\)</span> cost function by step-wise update of each model parameter <span class="math inline">\(x\)</span> as follows:</p>
<p><span class="math display">\[
\theta_{t+1}^i \leftarrow \theta_{t}^i - \alpha \frac{\partial J_{(\theta)}}{\partial \theta_i} 
\]</span></p>
<p>The parameters are the matrix coefficients <span class="math inline">\(p_{i,k}\)</span> <span class="math inline">\(q_{j,k}\)</span>. <span class="math inline">\(\alpha\)</span> is the learning parameter that needs to be adjusted.</p>
</div>
<div id="cost-function-partial-derivatives" class="section level4">
<h4><span class="header-section-number">6.3.0.2</span> Cost function partial derivatives</h4>
<p>The partial derivatives of the cost function is:</p>
<p><span class="math display">\[
\frac{\partial J_{P,Q}}{\partial x} = \frac{\partial}{\partial x} \left ( \sum_{(i, j) \in \Omega} {\left ( r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right )^2} + \frac{\lambda}{2} \left ( \sum_{i,k}{p_{i,k}^2} + \sum_{j,k}{q_{j,k}^2} \right ) \right )
\]</span></p>
<p><span class="math display">\[
\frac{\partial J_{P,Q}}{\partial x} = \sum_{(i, j) \in \Omega} { 2 \frac{\partial r_{i,j} - \sum_{k=1}^{N_{features}} {p_{i,k} q_{j,k}}} {\partial x} \left (  r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right )  } + \frac{\lambda}{2} \left ( \sum_{i,k}{2 \frac{\partial p_{i,k}}{\partial x} p_{i,k}} + \sum_{j,k}{2 \frac{\partial p_{i,k}}{\partial x} q_{j,k}} \right ) 
\]</span></p>
<p>We note that <span class="math inline">\(r_{i,j}\)</span> are constants</p>
<p><span class="math display">\[
\frac{\partial J_{P,Q}}{\partial x} = 2 \sum_{(i, j) \in \Omega} \sum_{k=1}^{N_{features}} \left ( { \frac{\partial - {p_{i,k} q_{j,k}}}{\partial x} \left (  r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right ) }  \right ) + \lambda \left ( \sum_{i,k}{\frac{\partial p_{i,k}}{\partial x} p_{i,k}} + \sum_{j,k}{\frac{\partial p_{i,k}}{\partial x} q_{j,k}} \right ) 
\]</span></p>
<p>If <span class="math inline">\(x\)</span> is a coefficient of <span class="math inline">\(P\)</span> (resp. <span class="math inline">\(Q\)</span>), say <span class="math inline">\(p_{a,b}\)</span> (resp. <span class="math inline">\(q_{a,b}\)</span>), all partial derivatives will be nil unless for <span class="math inline">\((i,j) = (a,b)\)</span>.</p>
<p>Therefore:</p>
<p><span class="math display">\[ 
\frac{\partial J_{P,Q}}{\partial p_{a,b}} = -2 \sum_{(i, j) \in \Omega} { q_{j,b} \left (  r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right )} + \lambda p_{a,b} 
\]</span></p>
<p>and,</p>
<p><span class="math display">\[
\frac{\partial J_{P,Q}}{\partial q_{a,b}} = -2 \sum_{(i, j) \in \Omega} { p_{i,b} \left (  r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}} \right ) } + \lambda q_{a,b} 
\]</span></p>
<p>Since <span class="math inline">\(\epsilon{i, j} = r_{i,j} - \sum_{k=1}^{N_{features}}{p_{i,k} q_{j,k}}\)</span> is the rating prediction error, this becomes:</p>
<p><span class="math display">\[ 
\frac{\partial J_{P,Q}}{\partial p_{a,b}} = -2 \sum_{(i, j) \in \Omega} { q_{j,b} \epsilon_{i,j}} + \lambda p_{a,b} 
\]</span></p>
<p>and,</p>
<p><span class="math display">\[
\frac{\partial J_{P,Q}}{\partial q_{a,b}} = -2 \sum_{(i, j) \in \Omega} { p_{i,b} \epsilon_{i,j} } + \lambda q_{a,b} 
\]</span></p>
<p>We note that the recent implementations of autodifferentiation algorithm (key to deep learning implementations) would avoid making those calculations by hand. We did not attempt to use the R <code>madness</code> package.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a></p>
</div>
<div id="stochastic-gradient-descent-sgd" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Stochastic Gradient Descent (SGD)</h3>
<p>The size of the datasets is prohibitive to do those calculations across the entire training set.</p>
<p>Instead, we will repeatedly update the model parameters on small random samples of the training set.</p>
<p>Chapter 14 of <span class="citation">(Shalev-Shwartz and Ben-David <a href="#ref-shalev2014understanding">2014</a>)</span> gives an extensive introduction to various SGD algorithms.</p>
<p>We implemented a simple version of the algorithm and present the code in more detail.</p>

<div class="sourceCode" id="cb90"><pre class="sourceCode numberSource r numberLines lineAnchors"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" href="#cb90-1" data-line-number="1"><span class="co"># The algorithm is implemented from scratch and relies on nothing but the `Tidyverse` libraries.</span></a>
<a class="sourceLine" id="cb90-2" href="#cb90-2" data-line-number="2"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb90-3" href="#cb90-3" data-line-number="3"></a>
<a class="sourceLine" id="cb90-4" href="#cb90-4" data-line-number="4"><span class="co"># The quality of the training and predictions is measured by the _root mean squared error_</span></a>
<a class="sourceLine" id="cb90-5" href="#cb90-5" data-line-number="5"><span class="co"># (RMSE), for which we define a few helper functions (the global variables are defined</span></a>
<a class="sourceLine" id="cb90-6" href="#cb90-6" data-line-number="6"><span class="co"># later):</span></a>
<a class="sourceLine" id="cb90-7" href="#cb90-7" data-line-number="7">rmse_training &lt;-<span class="st"> </span><span class="cf">function</span>() {</a>
<a class="sourceLine" id="cb90-8" href="#cb90-8" data-line-number="8">  prediction_Z &lt;-<span class="st"> </span><span class="kw">rowSums</span>(LF_Model<span class="op">$</span>P[tri_train<span class="op">$</span>userN,] <span class="op">*</span></a>
<a class="sourceLine" id="cb90-9" href="#cb90-9" data-line-number="9"><span class="st">                            </span>LF_Model<span class="op">$</span>Q[tri_train<span class="op">$</span>movieN,])</a>
<a class="sourceLine" id="cb90-10" href="#cb90-10" data-line-number="10">  prediction &lt;-<span class="st"> </span>prediction_Z <span class="op">*</span><span class="st"> </span>r_sd <span class="op">+</span><span class="st"> </span>r_m</a>
<a class="sourceLine" id="cb90-11" href="#cb90-11" data-line-number="11">  <span class="kw">sqrt</span>(<span class="kw">sum</span>((tri_train<span class="op">$</span>rating <span class="op">-</span><span class="st"> </span>prediction) <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>nSamples))</a>
<a class="sourceLine" id="cb90-12" href="#cb90-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb90-13" href="#cb90-13" data-line-number="13"></a>
<a class="sourceLine" id="cb90-14" href="#cb90-14" data-line-number="14">rmse_validation &lt;-<span class="st"> </span><span class="cf">function</span>() {</a>
<a class="sourceLine" id="cb90-15" href="#cb90-15" data-line-number="15">  prediction_Z &lt;-<span class="st"> </span><span class="kw">rowSums</span>(LF_Model<span class="op">$</span>P[tri_test<span class="op">$</span>userN,] <span class="op">*</span></a>
<a class="sourceLine" id="cb90-16" href="#cb90-16" data-line-number="16"><span class="st">                            </span>LF_Model<span class="op">$</span>Q[tri_test<span class="op">$</span>movieN,])</a>
<a class="sourceLine" id="cb90-17" href="#cb90-17" data-line-number="17">  prediction &lt;-<span class="st"> </span>prediction_Z <span class="op">*</span><span class="st"> </span>r_sd <span class="op">+</span><span class="st"> </span>r_m</a>
<a class="sourceLine" id="cb90-18" href="#cb90-18" data-line-number="18">  <span class="kw">sqrt</span>(<span class="kw">sum</span>((tri_test<span class="op">$</span>rating <span class="op">-</span><span class="st"> </span>prediction) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>nTest)</a>
<a class="sourceLine" id="cb90-19" href="#cb90-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb90-20" href="#cb90-20" data-line-number="20"></a>
<a class="sourceLine" id="cb90-21" href="#cb90-21" data-line-number="21">sum_square &lt;-<span class="st"> </span><span class="cf">function</span>(v) {</a>
<a class="sourceLine" id="cb90-22" href="#cb90-22" data-line-number="22">  <span class="kw">return</span> (<span class="kw">sqrt</span>(<span class="kw">sum</span>(v <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(v)))</a>
<a class="sourceLine" id="cb90-23" href="#cb90-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb90-24" href="#cb90-24" data-line-number="24"></a>
<a class="sourceLine" id="cb90-25" href="#cb90-25" data-line-number="25"><span class="co"># The key function updates the model coefficients. Its inputs are:</span></a>
<a class="sourceLine" id="cb90-26" href="#cb90-26" data-line-number="26"><span class="co">#</span></a>
<a class="sourceLine" id="cb90-27" href="#cb90-27" data-line-number="27"><span class="co"># + a list that contains the $P$ an $Q$ matrices, the training RMSE of those matrices, and</span></a>
<a class="sourceLine" id="cb90-28" href="#cb90-28" data-line-number="28"><span class="co"># a logical value indicating whether this RMSE is worse than what it was before the update</span></a>
<a class="sourceLine" id="cb90-29" href="#cb90-29" data-line-number="29"><span class="co"># (i.e. did the update diverge).</span></a>
<a class="sourceLine" id="cb90-30" href="#cb90-30" data-line-number="30"><span class="co">#</span></a>
<a class="sourceLine" id="cb90-31" href="#cb90-31" data-line-number="31"><span class="co"># + a `batch_size` that defines the number of samples to be drawn from the training set. A</span></a>
<a class="sourceLine" id="cb90-32" href="#cb90-32" data-line-number="32"><span class="co"># normal gradient descent would use the full training set; by default we only use 10,000</span></a>
<a class="sourceLine" id="cb90-33" href="#cb90-33" data-line-number="33"><span class="co"># samples out of 10 million (one tenth of a percent).</span></a>
<a class="sourceLine" id="cb90-34" href="#cb90-34" data-line-number="34"><span class="co">#</span></a>
<a class="sourceLine" id="cb90-35" href="#cb90-35" data-line-number="35"><span class="co"># + The cost regularisation `lambda` and gradient descent learning parameter `alpha`.</span></a>
<a class="sourceLine" id="cb90-36" href="#cb90-36" data-line-number="36"><span class="co">#</span></a>
<a class="sourceLine" id="cb90-37" href="#cb90-37" data-line-number="37"><span class="co"># + A number of `times` to run the descent before recalculating the RMSE and exiting the</span></a>
<a class="sourceLine" id="cb90-38" href="#cb90-38" data-line-number="38"><span class="co"># function (calculating the RMSE is computationally expensive).</span></a>
<a class="sourceLine" id="cb90-39" href="#cb90-39" data-line-number="39"><span class="co">#</span></a>
<a class="sourceLine" id="cb90-40" href="#cb90-40" data-line-number="40"><span class="co">#</span></a>
<a class="sourceLine" id="cb90-41" href="#cb90-41" data-line-number="41"><span class="co"># The training set used is less rich than the original set. As discussed, it only uses the</span></a>
<a class="sourceLine" id="cb90-42" href="#cb90-42" data-line-number="42"><span class="co"># rating (more exactly on the z_score of the rating). Genres, timestamps,... are</span></a>
<a class="sourceLine" id="cb90-43" href="#cb90-43" data-line-number="43"><span class="co"># discarded.</span></a>
<a class="sourceLine" id="cb90-44" href="#cb90-44" data-line-number="44"></a>
<a class="sourceLine" id="cb90-45" href="#cb90-45" data-line-number="45"></a>
<a class="sourceLine" id="cb90-46" href="#cb90-46" data-line-number="46"><span class="co"># Iterate gradient descent</span></a>
<a class="sourceLine" id="cb90-47" href="#cb90-47" data-line-number="47">stochastic_grad_descent &lt;-<span class="st"> </span><span class="cf">function</span>(model,</a>
<a class="sourceLine" id="cb90-48" href="#cb90-48" data-line-number="48">                                    <span class="dt">times =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb90-49" href="#cb90-49" data-line-number="49">                                    <span class="dt">batch_size =</span> <span class="dv">10000</span>,</a>
<a class="sourceLine" id="cb90-50" href="#cb90-50" data-line-number="50">                                    <span class="dt">lambda =</span> <span class="fl">0.1</span>,</a>
<a class="sourceLine" id="cb90-51" href="#cb90-51" data-line-number="51">                                    <span class="dt">alpha =</span> <span class="fl">0.01</span>,</a>
<a class="sourceLine" id="cb90-52" href="#cb90-52" data-line-number="52">                                    <span class="dt">verbose =</span> <span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb90-53" href="#cb90-53" data-line-number="53">  <span class="co"># Run the descent `times` times.</span></a>
<a class="sourceLine" id="cb90-54" href="#cb90-54" data-line-number="54">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>times) {</a>
<a class="sourceLine" id="cb90-55" href="#cb90-55" data-line-number="55">    <span class="co"># Extract a sample of size `batch_size` from the training set.</span></a>
<a class="sourceLine" id="cb90-56" href="#cb90-56" data-line-number="56">    spl &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>nSamples, <span class="dt">size =</span> batch_size, <span class="dt">replace =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb90-57" href="#cb90-57" data-line-number="57">    spl_training_values &lt;-<span class="st"> </span>tri_train[spl,]</a>
<a class="sourceLine" id="cb90-58" href="#cb90-58" data-line-number="58">    </a>
<a class="sourceLine" id="cb90-59" href="#cb90-59" data-line-number="59">    </a>
<a class="sourceLine" id="cb90-60" href="#cb90-60" data-line-number="60">    <span class="co"># Take a subset of `P` and `Q` matching the users and</span></a>
<a class="sourceLine" id="cb90-61" href="#cb90-61" data-line-number="61">    <span class="co"># movies in the training sample.</span></a>
<a class="sourceLine" id="cb90-62" href="#cb90-62" data-line-number="62">    spl_P &lt;-<span class="st"> </span>model<span class="op">$</span>P[spl_training_values<span class="op">$</span>userN,]</a>
<a class="sourceLine" id="cb90-63" href="#cb90-63" data-line-number="63">    spl_Q &lt;-<span class="st"> </span>model<span class="op">$</span>Q[spl_training_values<span class="op">$</span>movieN,]</a>
<a class="sourceLine" id="cb90-64" href="#cb90-64" data-line-number="64">    </a>
<a class="sourceLine" id="cb90-65" href="#cb90-65" data-line-number="65">    <span class="co"># rowSums returns the cross-product for a given user and movie.</span></a>
<a class="sourceLine" id="cb90-66" href="#cb90-66" data-line-number="66">    <span class="co"># err is the term inside brackets in the partial derivatives</span></a>
<a class="sourceLine" id="cb90-67" href="#cb90-67" data-line-number="67">    <span class="co"># calculation above.</span></a>
<a class="sourceLine" id="cb90-68" href="#cb90-68" data-line-number="68">    err &lt;-<span class="st"> </span>spl_training_values<span class="op">$</span>rating_z <span class="op">-</span><span class="st"> </span><span class="kw">rowSums</span>(spl_P <span class="op">*</span><span class="st"> </span>spl_Q)</a>
<a class="sourceLine" id="cb90-69" href="#cb90-69" data-line-number="69">    </a>
<a class="sourceLine" id="cb90-70" href="#cb90-70" data-line-number="70">    <span class="co"># Partial derivatives wrt p and q</span></a>
<a class="sourceLine" id="cb90-71" href="#cb90-71" data-line-number="71">    delta_P &lt;-<span class="st"> </span><span class="op">-</span>err <span class="op">*</span><span class="st"> </span>spl_Q <span class="op">+</span><span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span>spl_P</a>
<a class="sourceLine" id="cb90-72" href="#cb90-72" data-line-number="72">    delta_Q &lt;-<span class="st"> </span><span class="op">-</span>err <span class="op">*</span><span class="st"> </span>spl_P <span class="op">+</span><span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span>spl_Q</a>
<a class="sourceLine" id="cb90-73" href="#cb90-73" data-line-number="73">    </a>
<a class="sourceLine" id="cb90-74" href="#cb90-74" data-line-number="74">    model<span class="op">$</span>P[spl_training_values<span class="op">$</span>userN,]  &lt;-<span class="st"> </span>spl_P <span class="op">-</span><span class="st"> </span>alpha <span class="op">*</span><span class="st"> </span>delta_P</a>
<a class="sourceLine" id="cb90-75" href="#cb90-75" data-line-number="75">    model<span class="op">$</span>Q[spl_training_values<span class="op">$</span>movieN,] &lt;-<span class="st"> </span>spl_Q <span class="op">-</span><span class="st"> </span>alpha <span class="op">*</span><span class="st"> </span>delta_Q</a>
<a class="sourceLine" id="cb90-76" href="#cb90-76" data-line-number="76">    </a>
<a class="sourceLine" id="cb90-77" href="#cb90-77" data-line-number="77">  }</a>
<a class="sourceLine" id="cb90-78" href="#cb90-78" data-line-number="78">  </a>
<a class="sourceLine" id="cb90-79" href="#cb90-79" data-line-number="79">  <span class="co"># RMSE against the training set</span></a>
<a class="sourceLine" id="cb90-80" href="#cb90-80" data-line-number="80">  error &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((</a>
<a class="sourceLine" id="cb90-81" href="#cb90-81" data-line-number="81">    tri_train<span class="op">$</span>rating_z <span class="op">-</span><span class="st"> </span><span class="kw">rowSums</span>(model<span class="op">$</span>P[tri_train<span class="op">$</span>userN,] <span class="op">*</span></a>
<a class="sourceLine" id="cb90-82" href="#cb90-82" data-line-number="82"><span class="st">                                   </span>model<span class="op">$</span>Q[tri_train<span class="op">$</span>movieN,])</a>
<a class="sourceLine" id="cb90-83" href="#cb90-83" data-line-number="83">  ) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb90-84" href="#cb90-84" data-line-number="84">  <span class="op">/</span><span class="st"> </span>nSamples)</a>
<a class="sourceLine" id="cb90-85" href="#cb90-85" data-line-number="85">  </a>
<a class="sourceLine" id="cb90-86" href="#cb90-86" data-line-number="86">  <span class="co"># Compares to RMSE before update</span></a>
<a class="sourceLine" id="cb90-87" href="#cb90-87" data-line-number="87">  model<span class="op">$</span>WORSE_RMSE &lt;-<span class="st"> </span>(model<span class="op">$</span>RMSE <span class="op">&lt;</span><span class="st"> </span>error)</a>
<a class="sourceLine" id="cb90-88" href="#cb90-88" data-line-number="88">  model<span class="op">$</span>RMSE &lt;-<span class="st"> </span>error</a>
<a class="sourceLine" id="cb90-89" href="#cb90-89" data-line-number="89">  </a>
<a class="sourceLine" id="cb90-90" href="#cb90-90" data-line-number="90">  <span class="co"># Print some information to keep track of success</span></a>
<a class="sourceLine" id="cb90-91" href="#cb90-91" data-line-number="91">  <span class="cf">if</span> (verbose) {</a>
<a class="sourceLine" id="cb90-92" href="#cb90-92" data-line-number="92">    <span class="kw">cat</span>(</a>
<a class="sourceLine" id="cb90-93" href="#cb90-93" data-line-number="93">      <span class="st">&quot;  # features=&quot;</span>,</a>
<a class="sourceLine" id="cb90-94" href="#cb90-94" data-line-number="94">      <span class="kw">ncol</span>(model<span class="op">$</span>P),</a>
<a class="sourceLine" id="cb90-95" href="#cb90-95" data-line-number="95">      <span class="st">&quot;  J=&quot;</span>,</a>
<a class="sourceLine" id="cb90-96" href="#cb90-96" data-line-number="96">      nSamples <span class="op">*</span><span class="st"> </span>error <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span></a>
<a class="sourceLine" id="cb90-97" href="#cb90-97" data-line-number="97"><span class="st">        </span>lambda <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sum</span>(model<span class="op">$</span>P <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(model<span class="op">$</span>Q <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)),</a>
<a class="sourceLine" id="cb90-98" href="#cb90-98" data-line-number="98">      <span class="st">&quot;  Z-scores RMSE=&quot;</span>,</a>
<a class="sourceLine" id="cb90-99" href="#cb90-99" data-line-number="99">      model<span class="op">$</span>RMSE,</a>
<a class="sourceLine" id="cb90-100" href="#cb90-100" data-line-number="100">      <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span></a>
<a class="sourceLine" id="cb90-101" href="#cb90-101" data-line-number="101">    )</a>
<a class="sourceLine" id="cb90-102" href="#cb90-102" data-line-number="102">    <span class="kw">flush.console</span>()</a>
<a class="sourceLine" id="cb90-103" href="#cb90-103" data-line-number="103">  }</a>
<a class="sourceLine" id="cb90-104" href="#cb90-104" data-line-number="104">  </a>
<a class="sourceLine" id="cb90-105" href="#cb90-105" data-line-number="105">  <span class="kw">return</span>(model)</a>
<a class="sourceLine" id="cb90-106" href="#cb90-106" data-line-number="106">}</a>
<a class="sourceLine" id="cb90-107" href="#cb90-107" data-line-number="107"></a>
<a class="sourceLine" id="cb90-108" href="#cb90-108" data-line-number="108"></a>
<a class="sourceLine" id="cb90-109" href="#cb90-109" data-line-number="109"><span class="kw">rm</span>(list_results)</a>
<a class="sourceLine" id="cb90-110" href="#cb90-110" data-line-number="110">list_results &lt;-<span class="st"> </span><span class="kw">tibble</span>(</a>
<a class="sourceLine" id="cb90-111" href="#cb90-111" data-line-number="111">  <span class="st">&quot;alpha&quot;</span> =<span class="st"> </span><span class="kw">numeric</span>(),</a>
<a class="sourceLine" id="cb90-112" href="#cb90-112" data-line-number="112">  <span class="st">&quot;lambda&quot;</span> =<span class="st"> </span><span class="kw">numeric</span>(),</a>
<a class="sourceLine" id="cb90-113" href="#cb90-113" data-line-number="113">  <span class="st">&quot;nFeatures&quot;</span> =<span class="st"> </span><span class="kw">numeric</span>(),</a>
<a class="sourceLine" id="cb90-114" href="#cb90-114" data-line-number="114">  <span class="st">&quot;rmse_training_z_score&quot;</span> =<span class="st"> </span><span class="kw">numeric</span>(),</a>
<a class="sourceLine" id="cb90-115" href="#cb90-115" data-line-number="115">  <span class="st">&quot;rmse_training&quot;</span> =<span class="st"> </span><span class="kw">numeric</span>(),</a>
<a class="sourceLine" id="cb90-116" href="#cb90-116" data-line-number="116">  <span class="st">&quot;rmse_validation&quot;</span> =<span class="st"> </span><span class="kw">numeric</span>()</a>
<a class="sourceLine" id="cb90-117" href="#cb90-117" data-line-number="117">)</a>
<a class="sourceLine" id="cb90-118" href="#cb90-118" data-line-number="118"></a>
<a class="sourceLine" id="cb90-119" href="#cb90-119" data-line-number="119"><span class="co"># The main training loop runs as follows:</span></a>
<a class="sourceLine" id="cb90-120" href="#cb90-120" data-line-number="120"><span class="co">#</span></a>
<a class="sourceLine" id="cb90-121" href="#cb90-121" data-line-number="121"><span class="co"># + We start with 3 features.</span></a>
<a class="sourceLine" id="cb90-122" href="#cb90-122" data-line-number="122"><span class="co">#</span></a>
<a class="sourceLine" id="cb90-123" href="#cb90-123" data-line-number="123"><span class="co"># + The model is updated in batches of 100 updates. This is done up to 250 times. At each</span></a>
<a class="sourceLine" id="cb90-124" href="#cb90-124" data-line-number="124"><span class="co"># time, if the model starts diverging, the learning parameter ($\alpha$) is reduced.</span></a>
<a class="sourceLine" id="cb90-125" href="#cb90-125" data-line-number="125"><span class="co">#</span></a>
<a class="sourceLine" id="cb90-126" href="#cb90-126" data-line-number="126"><span class="co"># + Once the 250 times have passed, or if $\alpha$ has become incredibly small, or if the</span></a>
<a class="sourceLine" id="cb90-127" href="#cb90-127" data-line-number="127"><span class="co"># RMSE doesn&#39;t really improve anymoe (by less than 1 millionth), we add another features</span></a>
<a class="sourceLine" id="cb90-128" href="#cb90-128" data-line-number="128"><span class="co"># and start again.</span></a>
<a class="sourceLine" id="cb90-129" href="#cb90-129" data-line-number="129"></a>
<a class="sourceLine" id="cb90-130" href="#cb90-130" data-line-number="130">initial_alpha &lt;-<span class="st"> </span><span class="fl">0.1</span></a>
<a class="sourceLine" id="cb90-131" href="#cb90-131" data-line-number="131"><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {</a>
<a class="sourceLine" id="cb90-132" href="#cb90-132" data-line-number="132">  <span class="co"># Current number of features</span></a>
<a class="sourceLine" id="cb90-133" href="#cb90-133" data-line-number="133">  number_features &lt;-<span class="st"> </span><span class="kw">ncol</span>(LF_Model<span class="op">$</span>P)</a>
<a class="sourceLine" id="cb90-134" href="#cb90-134" data-line-number="134">  </a>
<a class="sourceLine" id="cb90-135" href="#cb90-135" data-line-number="135">  <span class="co"># lambda = 0.01 for 25 features, i.e. for about 2,000,000 parameters.</span></a>
<a class="sourceLine" id="cb90-136" href="#cb90-136" data-line-number="136">  <span class="co"># We keep lambda proportional to the number of features</span></a>
<a class="sourceLine" id="cb90-137" href="#cb90-137" data-line-number="137">  lambda &lt;-<span class="st"> </span><span class="fl">0.1</span> <span class="op">*</span><span class="st"> </span>(nUsers <span class="op">+</span><span class="st"> </span>nMovies) <span class="op">*</span><span class="st"> </span>number_features <span class="op">/</span><span class="st"> </span><span class="dv">2000000</span></a>
<a class="sourceLine" id="cb90-138" href="#cb90-138" data-line-number="138">  </a>
<a class="sourceLine" id="cb90-139" href="#cb90-139" data-line-number="139">  alpha &lt;-<span class="st"> </span>initial_alpha</a>
<a class="sourceLine" id="cb90-140" href="#cb90-140" data-line-number="140">  </a>
<a class="sourceLine" id="cb90-141" href="#cb90-141" data-line-number="141">  <span class="kw">cat</span>(</a>
<a class="sourceLine" id="cb90-142" href="#cb90-142" data-line-number="142">    <span class="st">&quot;CURRENT FEATURES: &quot;</span>,</a>
<a class="sourceLine" id="cb90-143" href="#cb90-143" data-line-number="143">    number_features,</a>
<a class="sourceLine" id="cb90-144" href="#cb90-144" data-line-number="144">    <span class="st">&quot;---- Pre-training validation RMSE = &quot;</span>,</a>
<a class="sourceLine" id="cb90-145" href="#cb90-145" data-line-number="145">    <span class="kw">rmse_validation</span>(),</a>
<a class="sourceLine" id="cb90-146" href="#cb90-146" data-line-number="146">    <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span></a>
<a class="sourceLine" id="cb90-147" href="#cb90-147" data-line-number="147">  )</a>
<a class="sourceLine" id="cb90-148" href="#cb90-148" data-line-number="148">  </a>
<a class="sourceLine" id="cb90-149" href="#cb90-149" data-line-number="149">  list_results &lt;-<span class="st"> </span>list_results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">add_row</span>(</a>
<a class="sourceLine" id="cb90-150" href="#cb90-150" data-line-number="150">    <span class="dt">alpha =</span> alpha,</a>
<a class="sourceLine" id="cb90-151" href="#cb90-151" data-line-number="151">    <span class="dt">lambda =</span> lambda,</a>
<a class="sourceLine" id="cb90-152" href="#cb90-152" data-line-number="152">    <span class="dt">nFeatures =</span> number_features,</a>
<a class="sourceLine" id="cb90-153" href="#cb90-153" data-line-number="153">    <span class="dt">rmse_training_z_score =</span> LF_Model<span class="op">$</span>RMSE,</a>
<a class="sourceLine" id="cb90-154" href="#cb90-154" data-line-number="154">    <span class="dt">rmse_training =</span> <span class="kw">rmse_training</span>(),</a>
<a class="sourceLine" id="cb90-155" href="#cb90-155" data-line-number="155">    <span class="dt">rmse_validation =</span> <span class="kw">rmse_validation</span>()</a>
<a class="sourceLine" id="cb90-156" href="#cb90-156" data-line-number="156">  )</a>
<a class="sourceLine" id="cb90-157" href="#cb90-157" data-line-number="157">  </a>
<a class="sourceLine" id="cb90-158" href="#cb90-158" data-line-number="158">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">250</span>) {</a>
<a class="sourceLine" id="cb90-159" href="#cb90-159" data-line-number="159">    pre_RMSE &lt;-<span class="st"> </span>LF_Model<span class="op">$</span>RMSE</a>
<a class="sourceLine" id="cb90-160" href="#cb90-160" data-line-number="160">    LF_Model &lt;-<span class="st"> </span><span class="kw">stochastic_grad_descent</span>(</a>
<a class="sourceLine" id="cb90-161" href="#cb90-161" data-line-number="161">      <span class="dt">model =</span> LF_Model,</a>
<a class="sourceLine" id="cb90-162" href="#cb90-162" data-line-number="162">      <span class="dt">times =</span> <span class="dv">100</span>,</a>
<a class="sourceLine" id="cb90-163" href="#cb90-163" data-line-number="163">      <span class="dt">batch_size =</span> <span class="dv">1000</span> <span class="op">*</span><span class="st"> </span>number_features,</a>
<a class="sourceLine" id="cb90-164" href="#cb90-164" data-line-number="164">      <span class="dt">alpha =</span> alpha,</a>
<a class="sourceLine" id="cb90-165" href="#cb90-165" data-line-number="165">      <span class="dt">lambda =</span> lambda</a>
<a class="sourceLine" id="cb90-166" href="#cb90-166" data-line-number="166">    )</a>
<a class="sourceLine" id="cb90-167" href="#cb90-167" data-line-number="167">    </a>
<a class="sourceLine" id="cb90-168" href="#cb90-168" data-line-number="168">    list_results &lt;-<span class="st"> </span>list_results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">add_row</span>(</a>
<a class="sourceLine" id="cb90-169" href="#cb90-169" data-line-number="169">      <span class="dt">alpha =</span> alpha,</a>
<a class="sourceLine" id="cb90-170" href="#cb90-170" data-line-number="170">      <span class="dt">lambda =</span> lambda,</a>
<a class="sourceLine" id="cb90-171" href="#cb90-171" data-line-number="171">      <span class="dt">nFeatures =</span> number_features,</a>
<a class="sourceLine" id="cb90-172" href="#cb90-172" data-line-number="172">      <span class="dt">rmse_training_z_score =</span> LF_Model<span class="op">$</span>RMSE,</a>
<a class="sourceLine" id="cb90-173" href="#cb90-173" data-line-number="173">      <span class="dt">rmse_training =</span> <span class="kw">rmse_training</span>(),</a>
<a class="sourceLine" id="cb90-174" href="#cb90-174" data-line-number="174">      <span class="dt">rmse_validation =</span> <span class="kw">rmse_validation</span>()</a>
<a class="sourceLine" id="cb90-175" href="#cb90-175" data-line-number="175">    )</a>
<a class="sourceLine" id="cb90-176" href="#cb90-176" data-line-number="176">    </a>
<a class="sourceLine" id="cb90-177" href="#cb90-177" data-line-number="177">    <span class="cf">if</span> (LF_Model<span class="op">$</span>WORSE_RMSE) {</a>
<a class="sourceLine" id="cb90-178" href="#cb90-178" data-line-number="178">      alpha &lt;-<span class="st"> </span>alpha <span class="op">/</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb90-179" href="#cb90-179" data-line-number="179">      <span class="kw">cat</span>(<span class="st">&quot;Decreasing gradient parameter to: &quot;</span>, alpha, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</a>
<a class="sourceLine" id="cb90-180" href="#cb90-180" data-line-number="180">    }</a>
<a class="sourceLine" id="cb90-181" href="#cb90-181" data-line-number="181">    </a>
<a class="sourceLine" id="cb90-182" href="#cb90-182" data-line-number="182">    <span class="cf">if</span> (initial_alpha <span class="op">/</span><span class="st"> </span>alpha <span class="op">&gt;</span><span class="st"> </span><span class="dv">1000</span> <span class="op">|</span></a>
<a class="sourceLine" id="cb90-183" href="#cb90-183" data-line-number="183"><span class="st">        </span><span class="kw">abs</span>((LF_Model<span class="op">$</span>RMSE <span class="op">-</span><span class="st"> </span>pre_RMSE) <span class="op">/</span><span class="st"> </span>pre_RMSE) <span class="op">&lt;</span><span class="st"> </span><span class="fl">1e-6</span>) {</a>
<a class="sourceLine" id="cb90-184" href="#cb90-184" data-line-number="184">      <span class="cf">break</span>()</a>
<a class="sourceLine" id="cb90-185" href="#cb90-185" data-line-number="185">    }</a>
<a class="sourceLine" id="cb90-186" href="#cb90-186" data-line-number="186">  }</a>
<a class="sourceLine" id="cb90-187" href="#cb90-187" data-line-number="187">  </a>
<a class="sourceLine" id="cb90-188" href="#cb90-188" data-line-number="188">  </a>
<a class="sourceLine" id="cb90-189" href="#cb90-189" data-line-number="189">  <span class="co"># RMSE against validation set:</span></a>
<a class="sourceLine" id="cb90-190" href="#cb90-190" data-line-number="190">  rmse_validation_post &lt;-<span class="st"> </span><span class="kw">rmse_validation</span>()</a>
<a class="sourceLine" id="cb90-191" href="#cb90-191" data-line-number="191">  <span class="kw">cat</span>(</a>
<a class="sourceLine" id="cb90-192" href="#cb90-192" data-line-number="192">    <span class="st">&quot;CURRENT FEATURES: &quot;</span>,</a>
<a class="sourceLine" id="cb90-193" href="#cb90-193" data-line-number="193">    number_features,</a>
<a class="sourceLine" id="cb90-194" href="#cb90-194" data-line-number="194">    <span class="st">&quot;---- POST-training validation RMSE = &quot;</span>,</a>
<a class="sourceLine" id="cb90-195" href="#cb90-195" data-line-number="195">    rmse_validation_post,</a>
<a class="sourceLine" id="cb90-196" href="#cb90-196" data-line-number="196">    <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span></a>
<a class="sourceLine" id="cb90-197" href="#cb90-197" data-line-number="197">  )</a>
<a class="sourceLine" id="cb90-198" href="#cb90-198" data-line-number="198">  </a>
<a class="sourceLine" id="cb90-199" href="#cb90-199" data-line-number="199">  <span class="co"># if (number_features == 12){</span></a>
<a class="sourceLine" id="cb90-200" href="#cb90-200" data-line-number="200">  <span class="co">#   break()</span></a>
<a class="sourceLine" id="cb90-201" href="#cb90-201" data-line-number="201">  <span class="co"># }</span></a>
<a class="sourceLine" id="cb90-202" href="#cb90-202" data-line-number="202">  </a>
<a class="sourceLine" id="cb90-203" href="#cb90-203" data-line-number="203">  </a>
<a class="sourceLine" id="cb90-204" href="#cb90-204" data-line-number="204">  <span class="co"># Add k features</span></a>
<a class="sourceLine" id="cb90-205" href="#cb90-205" data-line-number="205">  k_features &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb90-206" href="#cb90-206" data-line-number="206">  LF_Model<span class="op">$</span>P &lt;-<span class="st"> </span><span class="kw">cbind</span>(LF_Model<span class="op">$</span>P,</a>
<a class="sourceLine" id="cb90-207" href="#cb90-207" data-line-number="207">                      <span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb90-208" href="#cb90-208" data-line-number="208">                        <span class="kw">rnorm</span>(</a>
<a class="sourceLine" id="cb90-209" href="#cb90-209" data-line-number="209">                          <span class="kw">nrow</span>(LF_Model<span class="op">$</span>P) <span class="op">*</span><span class="st"> </span>k_features,</a>
<a class="sourceLine" id="cb90-210" href="#cb90-210" data-line-number="210">                          <span class="dt">mean =</span> <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb90-211" href="#cb90-211" data-line-number="211">                          <span class="dt">sd =</span> <span class="kw">sd</span>(LF_Model<span class="op">$</span>P) <span class="op">/</span><span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb90-212" href="#cb90-212" data-line-number="212">                        ),</a>
<a class="sourceLine" id="cb90-213" href="#cb90-213" data-line-number="213">                        <span class="dt">nrow =</span> <span class="kw">nrow</span>(LF_Model<span class="op">$</span>P),</a>
<a class="sourceLine" id="cb90-214" href="#cb90-214" data-line-number="214">                        <span class="dt">ncol =</span> k_features</a>
<a class="sourceLine" id="cb90-215" href="#cb90-215" data-line-number="215">                      ))</a>
<a class="sourceLine" id="cb90-216" href="#cb90-216" data-line-number="216">  </a>
<a class="sourceLine" id="cb90-217" href="#cb90-217" data-line-number="217">  LF_Model<span class="op">$</span>Q &lt;-<span class="st"> </span><span class="kw">cbind</span>(LF_Model<span class="op">$</span>Q,</a>
<a class="sourceLine" id="cb90-218" href="#cb90-218" data-line-number="218">                      <span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb90-219" href="#cb90-219" data-line-number="219">                        <span class="kw">rnorm</span>(</a>
<a class="sourceLine" id="cb90-220" href="#cb90-220" data-line-number="220">                          <span class="kw">nrow</span>(LF_Model<span class="op">$</span>Q) <span class="op">*</span><span class="st"> </span>k_features,</a>
<a class="sourceLine" id="cb90-221" href="#cb90-221" data-line-number="221">                          <span class="dt">mean =</span> <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb90-222" href="#cb90-222" data-line-number="222">                          <span class="dt">sd =</span> <span class="kw">sd</span>(LF_Model<span class="op">$</span>Q) <span class="op">/</span><span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb90-223" href="#cb90-223" data-line-number="223">                        ),</a>
<a class="sourceLine" id="cb90-224" href="#cb90-224" data-line-number="224">                        <span class="dt">nrow =</span> <span class="kw">nrow</span>(LF_Model<span class="op">$</span>Q),</a>
<a class="sourceLine" id="cb90-225" href="#cb90-225" data-line-number="225">                        <span class="dt">ncol =</span> k_features</a>
<a class="sourceLine" id="cb90-226" href="#cb90-226" data-line-number="226">                      ))</a>
<a class="sourceLine" id="cb90-227" href="#cb90-227" data-line-number="227">  </a>
<a class="sourceLine" id="cb90-228" href="#cb90-228" data-line-number="228">}</a>
<a class="sourceLine" id="cb90-229" href="#cb90-229" data-line-number="229"></a>
<a class="sourceLine" id="cb90-230" href="#cb90-230" data-line-number="230"><span class="kw">saveRDS</span>(list_results, <span class="st">&quot;datasets/LRMF_results.rds&quot;</span>)</a></code></pre></div>

</div>
</div>
<div id="various-errands" class="section level2">
<h2><span class="header-section-number">6.4</span> Various errands</h2>
<div id="optimising-the-scorecard-or-rating-depending-on-the-npv-curve." class="section level3">
<h3><span class="header-section-number">6.4.1</span> Optimising the scorecard (or rating) depending on the NPV curve.</h3>
<p>Input - Layer - Single Q - Distribution</p>
<p>Cost function as a function of the <span class="math inline">\(Q\)</span> parameter (equivalent to scorecard).</p>
<p>Looking back at the distribution of the NPV between the -1 and about 1.5, it is multimodal and looks like the sum of 4 log-normal distributions with modes centered on about</p>
<p><span class="math display">\[
PDF(x) = \frac{1}{x \sigma \sqrt{2 \pi}} exp{\left( -\frac{1}2 \left( \frac{ \log(x) - \mu }{\sigma} \right)^2 \right)}
\]</span></p>
<p><span class="math display">\[
\text{mode} = m = e^{\mu - \sigma^2} \text{, therefore: } \mu = log(\text{mode}) + \sigma^2
\]</span></p>
<p><span class="math display">\[
PDF(x) = \sqrt{\frac{e}{2 \pi}} \frac{1}{x \sigma}  exp{\left( -\frac{1}2 \left( \frac{\log(\frac{x}{m}) - \sigma^2}  {\sigma} \right)^2 \right)}
\]</span></p>
<p>We will center the distribution on the mode, therefore:</p>
<p><span class="math display">\[
PDF(x) = \sqrt{\frac{e}{2 \pi}} \frac{1}{\left( x - m \right) \sigma}  exp{ \left( -\frac{1}2 \left( \frac{\log(\frac{x - m}{m}) - \sigma^2}  {\sigma} \right)^2 \right) }
\]</span></p>
<p>The distributions tail is towards positive infinity. For the symmetric result, we would replace <span class="math inline">\(\left( x - m \right)\)</span> by <span class="math inline">\(-\left( x - m \right)\)</span>.</p>
<p>If we use 4 log-normal distributions, the cost function is:</p>
<p><span class="math display">\[
\operatorname{J}\left( x,Q\right) =-{{\left[ x-\left( \alpha_1 \operatorname{PDF_1}\left( x,Q\right) + \alpha_2 \operatorname{PDF_2}\left( x,Q\right) + \alpha_2 \operatorname{PDF_3}\left( x,Q\right) + \alpha_4 \operatorname{PDF_4}\left( x,Q\right) \right) \right] }^2}
\]</span></p>
<p>To optimise the shape of the total multi-modal distribution, we will assume that each <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(m\)</span> and <span class="math inline">\(\sigma\)</span> is a linear function of <span class="math inline">\(Q\)</span>. The derivative <span class="math inline">\(\frac{\partial{\operatorname{J}}}{\partial{Q}}\)</span> is<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a>:</p>
<p><span class="math display">\[
\begin{array}{ccc}
\frac{\partial{\operatorname{J}}}{\partial{Q}} \left(x, Q \right) = - \sqrt{\frac{2}{\pi}} x
&amp; - &amp; \sqrt{\frac{2}{\pi}} \frac{\alpha_1}{\sigma_1 \left(-x+m_1 \right)} {e^{-\frac{1}{2} \left( \frac{ \log{\left( \frac{-x+m_1}{ m_1} \right)} + \sigma_1^2}{\sigma_1} \right) ^2}} \\
&amp; - &amp; \frac{1}{\sqrt{2 \pi}} \frac{\alpha_2}{\sigma_2 \left(-x+m_2 \right)} {e^{-\frac{1}{2} \left( \frac{ \log{\left( \frac{-x+m_2}{ m_2} \right)} + \sigma_2^2}{\sigma_2} \right) ^2}} \\
&amp; - &amp; \sqrt{\frac{2}{\pi}} \frac{\alpha_3}{\sigma_3 \left(-x+m_3 \right)} {e^{-\frac{1}{2} \left( \frac{ \log{\left( \frac{-x+m_3}{ m_3} \right)} + \sigma_3^2}{\sigma_3} \right) ^2}} \\
&amp; - &amp; \sqrt{\frac{2}{\pi}} \frac{\alpha_4}{\sigma_4 \left( x-m_4 \right)} {e^{-\frac{1}{2} \left( \frac{ \log{\left( \frac{ x-m_4}{ m_4} \right)} + \sigma_4^2}{\sigma_4} \right) ^2}}
\end{array}
\]</span></p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-shalev2014understanding">
<p>Shalev-Shwartz, Shai, and Shai Ben-David. 2014. <em>Understanding Machine Learning: From Theory to Algorithms</em>. Cambridge university press. <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html" class="uri">https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="16">
<li id="fn16"><p>Source: <a href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" class="uri">https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html</a><a href="stochastic-gradient-descent.html#fnref16" class="footnote-back"></a></p></li>
<li id="fn17"><p><a href="https://github.com/shabbychef/madness" class="uri">https://github.com/shabbychef/madness</a><a href="stochastic-gradient-descent.html#fnref17" class="footnote-back"></a></p></li>
<li id="fn18"><p>This was actually generated using Maxima (code in appendix) which allows for quicker iterations.<a href="stochastic-gradient-descent.html#fnref18" class="footnote-back"></a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="errands-and-post-mortem.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Emmanuel-R8/HarvardX-LendingClub/edit/master/07-2-postmortem.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["HarvardX Credit Scoring of the LendingClub Dataset.pdf", "HarvardX Credit Scoring of the LendingClub Dataset.epub"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
